{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from keras.utils import np_utils#one hot\n",
    "import os\n",
    "from skimage import data\n",
    "from skimage import transform\n",
    "import skimage\n",
    "import numpy as np\n",
    "\n",
    "# Download the dataset\n",
    "\n",
    "\n",
    "def load_data(data_directory):\n",
    "    directories = [d for d in os.listdir(data_directory) \n",
    "                  if os.path.isdir(os.path.join(data_directory,d))]\n",
    "    labels=[]\n",
    "    images=[]\n",
    "    for d in directories:\n",
    "        label_directory = os.path.join(data_directory,d)\n",
    "        file_names = [os.path.join(label_directory,f)\n",
    "                     for f in os.listdir(label_directory)\n",
    "                     if f.endswith('.jpg')]\n",
    "        for f in file_names:\n",
    "            images.append(skimage.data.imread(f))\n",
    "            labels.append(str(d))\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ROOT_PATH = '/host//Can/dpr/'\n",
    "train_data_directory = os.path.join(ROOT_PATH, \"outPut3/train\")\n",
    "validation_data_directory = os.path.join(ROOT_PATH, \"outPut3/val\")\n",
    "test_data_directory = os.path.join(ROOT_PATH, \"outPut3/test\")\n",
    "train_images,train_labels = load_data(train_data_directory)\n",
    "test_images,test_labels = load_data(test_data_directory)\n",
    "validation_images,validation_labels = load_data(validation_data_directory)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#encoding trY teY\n",
    "from numpy import array\n",
    "#from numpy import argmax\n",
    "#from keras.utils import to_categorical\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define example\n",
    "\n",
    "#train_labels = np_utils.to_categorical(train_labels,num_classes = None)\n",
    "#test_labels = np_utils.to_categorical(test_labels,num_classes = None)\n",
    "trL,teL,valL=array(train_labels),array(test_labels),array(validation_labels)\n",
    "label_encoder = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_encoded_trL = label_encoder.fit_transform(trL)\n",
    "integer_encoded_teL = label_encoder.fit_transform(teL)\n",
    "integer_encoded_valL = label_encoder.fit_transform(valL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels = np_utils.to_categorical(integer_encoded_trL,num_classes = 32)\n",
    "testLabels = np_utils.to_categorical(integer_encoded_teL,num_classes = 32)\n",
    "validationLabels = np_utils.to_categorical(integer_encoded_valL,num_classes = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########convert rgb images into black and white\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "trainImages = rgb2gray(np.array(train_images))\n",
    "testImages = rgb2gray(np.array(test_images))\n",
    "valImages = rgb2gray(np.array(validation_images))\n",
    "train_flat_images = np.reshape(trainImages,(len(trainImages),16*8))\n",
    "test_flat_images = np.reshape(testImages,(len(testImages),16*8))\n",
    "val_flat_images = np.reshape(valImages,(len(valImages),16*8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411, 16, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valImages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_flat_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integer_encoded_trL[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validationLabels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Training-set:\t\t4446\n",
      "- Test-set:\t\t411\n",
      "- Validation-set:\t411\n"
     ]
    }
   ],
   "source": [
    "print('Size of:')\n",
    "print('- Training-set:\\t\\t{}'.format(len(trainLabels)))\n",
    "print('- Test-set:\\t\\t{}'.format(len(testLabels)))\n",
    "print('- Validation-set:\\t{}'.format(len(validation_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 8, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "learning_rate = 0.001  # The optimization learning rate\n",
    "epochs = 5  # Total number of training epochs\n",
    "batch_size = 1  # Training batch size\n",
    "display_freq = 10  # Frequency of displaying the training results\n",
    "trainNumber=len(train_labels)\n",
    "testNumber=len(test_labels)\n",
    "# Network Parameters\n",
    "# We know that MNIST images are 28 pixels in each dimension.\n",
    "img_h = 16\n",
    "img_w = 8\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_h * img_w\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "n_classes = 32\n",
    "\n",
    "# number of units in the first hidden layer\n",
    "h1 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight and bais wrappers\n",
    "def weight_variable(name, shape):\n",
    "    \"\"\"\n",
    "    Create a weight variable with appropriate initialization\n",
    "    name: weight name\n",
    "    shape: weight shape\n",
    "\n",
    "    return: initialized weight variable\n",
    "    \"\"\"\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('W_' + name,\n",
    "                           dtype=tf.float32,\n",
    "                           shape=shape,\n",
    "                           initializer=initer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(name, shape):\n",
    "    \"\"\"\n",
    "    Create a bias variable with appropriate initialization\n",
    "    name: bias variable name\n",
    "    shape: bias variable shape\n",
    "\n",
    "    return: initialized bias variable\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b_' + name,\n",
    "                           dtype=tf.float32,\n",
    "                           initializer=initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layer(x, num_nodes, name, use_relu=True):\n",
    "    \"\"\"\n",
    "    Creates a fully-connected layer\n",
    "    :param x: input from previous layer\n",
    "    :param num_nodes: number of hidden units in the fully-connected layer\n",
    "    :param name: layer name\n",
    "    :param use_relu: boolean to add ReLU non-linearity (or not)\n",
    "\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    in_dim = x.get_shape()[1]\n",
    "    W = weight_variable(name, shape=[in_dim, num_nodes])\n",
    "    b = bias_variable(name, [num_nodes])\n",
    "    layer = tf.matmul(x, W)\n",
    "    layer += b\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-1896dc37e3c5>:9: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create graph\n",
    "# Placeholders for inputs (x), outputs(y)\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_classes], name='Y')\n",
    "fc1 = fc_layer(x, h1, 'FC1', use_relu=True)\n",
    "output_logits = fc_layer(fc1, n_classes, 'OUT', use_relu=False)\n",
    "\n",
    "# Define the loss function, optimizer, and accuracy\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam-op').minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "\n",
    "# Network predictions\n",
    "cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "iter   0:\t Loss=3.45,\tTraining Accuracy=100.0%\n",
      "iter  10:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter  20:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter  30:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter  40:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter  50:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter  60:\t Loss=3.17,\tTraining Accuracy=100.0%\n",
      "iter  70:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter  80:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter  90:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 100:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 110:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 120:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 130:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 140:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 150:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 160:\t Loss=3.68,\tTraining Accuracy=0.0%\n",
      "iter 170:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 180:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter 190:\t Loss=3.56,\tTraining Accuracy=0.0%\n",
      "iter 200:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 210:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 220:\t Loss=3.66,\tTraining Accuracy=0.0%\n",
      "iter 230:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 240:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 250:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 260:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 270:\t Loss=3.59,\tTraining Accuracy=0.0%\n",
      "iter 280:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 290:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter 300:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 310:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 320:\t Loss=3.77,\tTraining Accuracy=0.0%\n",
      "iter 330:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 340:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 350:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 360:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter 370:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 380:\t Loss=3.33,\tTraining Accuracy=0.0%\n",
      "iter 390:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 400:\t Loss=3.68,\tTraining Accuracy=0.0%\n",
      "iter 410:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 420:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 430:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter 440:\t Loss=3.77,\tTraining Accuracy=0.0%\n",
      "iter 450:\t Loss=3.56,\tTraining Accuracy=0.0%\n",
      "iter 460:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter 470:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 480:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 490:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 500:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 510:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 520:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 530:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 540:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 550:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 560:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 570:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 580:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 590:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 600:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 610:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 620:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 630:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 640:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 650:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 660:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 670:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 680:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 690:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 700:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 710:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 720:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 730:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 740:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 750:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 760:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 770:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 780:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 790:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 800:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 810:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 820:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 830:\t Loss=3.39,\tTraining Accuracy=100.0%\n",
      "iter 840:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 850:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 860:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 870:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 880:\t Loss=3.40,\tTraining Accuracy=100.0%\n",
      "iter 890:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 900:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 910:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 920:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 930:\t Loss=3.33,\tTraining Accuracy=0.0%\n",
      "iter 940:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 950:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 960:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 970:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 980:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 990:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 1000:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 1010:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 1020:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 1030:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 1040:\t Loss=3.69,\tTraining Accuracy=0.0%\n",
      "iter 1050:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 1060:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 1070:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 1080:\t Loss=3.65,\tTraining Accuracy=0.0%\n",
      "iter 1090:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 1100:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 1110:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 1120:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 1130:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter 1140:\t Loss=3.80,\tTraining Accuracy=0.0%\n",
      "iter 1150:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 1160:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 1170:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 1180:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter 1190:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 1200:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 1210:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 1220:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 1230:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 1240:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 1250:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 1260:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 1270:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 1280:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 1290:\t Loss=3.33,\tTraining Accuracy=100.0%\n",
      "iter 1300:\t Loss=3.33,\tTraining Accuracy=0.0%\n",
      "iter 1310:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 1320:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 1330:\t Loss=3.21,\tTraining Accuracy=0.0%\n",
      "iter 1340:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 1350:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 1360:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 1370:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 1380:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 1390:\t Loss=3.64,\tTraining Accuracy=0.0%\n",
      "iter 1400:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 1410:\t Loss=3.29,\tTraining Accuracy=100.0%\n",
      "iter 1420:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 1430:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 1440:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 1450:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 1460:\t Loss=3.90,\tTraining Accuracy=0.0%\n",
      "iter 1470:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 1480:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 1490:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 1500:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 1510:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 1520:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 1530:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 1540:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 1550:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 1560:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter 1570:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 1580:\t Loss=3.21,\tTraining Accuracy=0.0%\n",
      "iter 1590:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 1600:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 1610:\t Loss=4.00,\tTraining Accuracy=0.0%\n",
      "iter 1620:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 1630:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 1640:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 1650:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 1660:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 1670:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 1680:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 1690:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 1700:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 1710:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 1720:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 1730:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 1740:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 1750:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 1760:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 1770:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 1780:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 1790:\t Loss=3.56,\tTraining Accuracy=0.0%\n",
      "iter 1800:\t Loss=3.59,\tTraining Accuracy=0.0%\n",
      "iter 1810:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 1820:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 1830:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 1840:\t Loss=3.56,\tTraining Accuracy=0.0%\n",
      "iter 1850:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 1860:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 1870:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 1880:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 1890:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 1900:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 1910:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter 1920:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter 1930:\t Loss=3.44,\tTraining Accuracy=0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1940:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 1950:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 1960:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 1970:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 1980:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 1990:\t Loss=3.56,\tTraining Accuracy=0.0%\n",
      "iter 2000:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter 2010:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 2020:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 2030:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 2040:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 2050:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 2060:\t Loss=3.69,\tTraining Accuracy=0.0%\n",
      "iter 2070:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 2080:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 2090:\t Loss=3.63,\tTraining Accuracy=0.0%\n",
      "iter 2100:\t Loss=3.70,\tTraining Accuracy=0.0%\n",
      "iter 2110:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 2120:\t Loss=3.56,\tTraining Accuracy=0.0%\n",
      "iter 2130:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 2140:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter 2150:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 2160:\t Loss=3.26,\tTraining Accuracy=100.0%\n",
      "iter 2170:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 2180:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 2190:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 2200:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 2210:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 2220:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 2230:\t Loss=3.24,\tTraining Accuracy=100.0%\n",
      "iter 2240:\t Loss=3.07,\tTraining Accuracy=100.0%\n",
      "iter 2250:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 2260:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 2270:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 2280:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 2290:\t Loss=2.88,\tTraining Accuracy=100.0%\n",
      "iter 2300:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 2310:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 2320:\t Loss=3.57,\tTraining Accuracy=0.0%\n",
      "iter 2330:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 2340:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 2350:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 2360:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 2370:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter 2380:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 2390:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter 2400:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 2410:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 2420:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 2430:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 2440:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 2450:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 2460:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 2470:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 2480:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 2490:\t Loss=3.20,\tTraining Accuracy=100.0%\n",
      "iter 2500:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 2510:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 2520:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 2530:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 2540:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 2550:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 2560:\t Loss=3.84,\tTraining Accuracy=0.0%\n",
      "iter 2570:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 2580:\t Loss=3.17,\tTraining Accuracy=100.0%\n",
      "iter 2590:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 2600:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 2610:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 2620:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 2630:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 2640:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 2650:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 2660:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 2670:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 2680:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 2690:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 2700:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 2710:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 2720:\t Loss=3.39,\tTraining Accuracy=100.0%\n",
      "iter 2730:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 2740:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter 2750:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 2760:\t Loss=3.05,\tTraining Accuracy=100.0%\n",
      "iter 2770:\t Loss=3.62,\tTraining Accuracy=0.0%\n",
      "iter 2780:\t Loss=3.56,\tTraining Accuracy=0.0%\n",
      "iter 2790:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 2800:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 2810:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 2820:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 2830:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 2840:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter 2850:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 2860:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 2870:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 2880:\t Loss=3.04,\tTraining Accuracy=0.0%\n",
      "iter 2890:\t Loss=3.66,\tTraining Accuracy=0.0%\n",
      "iter 2900:\t Loss=3.92,\tTraining Accuracy=0.0%\n",
      "iter 2910:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 2920:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 2930:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 2940:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 2950:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 2960:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 2970:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 2980:\t Loss=3.21,\tTraining Accuracy=0.0%\n",
      "iter 2990:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 3000:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 3010:\t Loss=3.64,\tTraining Accuracy=0.0%\n",
      "iter 3020:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 3030:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 3040:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 3050:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 3060:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 3070:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 3080:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 3090:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 3100:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter 3110:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 3120:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 3130:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 3140:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 3150:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 3160:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 3170:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 3180:\t Loss=3.64,\tTraining Accuracy=0.0%\n",
      "iter 3190:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 3200:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 3210:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 3220:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 3230:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 3240:\t Loss=3.57,\tTraining Accuracy=0.0%\n",
      "iter 3250:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 3260:\t Loss=3.04,\tTraining Accuracy=100.0%\n",
      "iter 3270:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 3280:\t Loss=3.78,\tTraining Accuracy=0.0%\n",
      "iter 3290:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 3300:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 3310:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 3320:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 3330:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 3340:\t Loss=3.58,\tTraining Accuracy=0.0%\n",
      "iter 3350:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 3360:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 3370:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter 3380:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 3390:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter 3400:\t Loss=2.91,\tTraining Accuracy=100.0%\n",
      "iter 3410:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 3420:\t Loss=3.80,\tTraining Accuracy=0.0%\n",
      "iter 3430:\t Loss=3.74,\tTraining Accuracy=0.0%\n",
      "iter 3440:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 3450:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 3460:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 3470:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 3480:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 3490:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 3500:\t Loss=2.94,\tTraining Accuracy=0.0%\n",
      "iter 3510:\t Loss=3.94,\tTraining Accuracy=0.0%\n",
      "iter 3520:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 3530:\t Loss=2.37,\tTraining Accuracy=100.0%\n",
      "iter 3540:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 3550:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 3560:\t Loss=3.10,\tTraining Accuracy=0.0%\n",
      "iter 3570:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter 3580:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 3590:\t Loss=2.85,\tTraining Accuracy=100.0%\n",
      "iter 3600:\t Loss=3.73,\tTraining Accuracy=0.0%\n",
      "iter 3610:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 3620:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 3630:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 3640:\t Loss=3.76,\tTraining Accuracy=0.0%\n",
      "iter 3650:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 3660:\t Loss=4.07,\tTraining Accuracy=0.0%\n",
      "iter 3670:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 3680:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 3690:\t Loss=2.80,\tTraining Accuracy=0.0%\n",
      "iter 3700:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 3710:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 3720:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 3730:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 3740:\t Loss=2.97,\tTraining Accuracy=0.0%\n",
      "iter 3750:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 3760:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 3770:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 3780:\t Loss=3.36,\tTraining Accuracy=0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3790:\t Loss=3.71,\tTraining Accuracy=0.0%\n",
      "iter 3800:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 3810:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 3820:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 3830:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 3840:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 3850:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 3860:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter 3870:\t Loss=3.64,\tTraining Accuracy=0.0%\n",
      "iter 3880:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 3890:\t Loss=2.68,\tTraining Accuracy=100.0%\n",
      "iter 3900:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 3910:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 3920:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter 3930:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 3940:\t Loss=3.04,\tTraining Accuracy=0.0%\n",
      "iter 3950:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 3960:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 3970:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 3980:\t Loss=3.77,\tTraining Accuracy=0.0%\n",
      "iter 3990:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 4000:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 4010:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 4020:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 4030:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 4040:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 4050:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 4060:\t Loss=3.06,\tTraining Accuracy=0.0%\n",
      "iter 4070:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 4080:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 4090:\t Loss=3.16,\tTraining Accuracy=100.0%\n",
      "iter 4100:\t Loss=3.01,\tTraining Accuracy=100.0%\n",
      "iter 4110:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter 4120:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 4130:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 4140:\t Loss=3.93,\tTraining Accuracy=0.0%\n",
      "iter 4150:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 4160:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 4170:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 4180:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 4190:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 4200:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 4210:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 4220:\t Loss=3.69,\tTraining Accuracy=0.0%\n",
      "iter 4230:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 4240:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 4250:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 4260:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 4270:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 4280:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 4290:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 4300:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 4310:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter 4320:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 4330:\t Loss=3.20,\tTraining Accuracy=0.0%\n",
      "iter 4340:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 4350:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 4360:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 4370:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 4380:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 4390:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 4400:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 4410:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter 4420:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 4430:\t Loss=3.16,\tTraining Accuracy=0.0%\n",
      "iter 4440:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 1, validation loss: 3.40, validation accuracy: 4.6%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 2\n",
      "iter   0:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter  10:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter  20:\t Loss=4.01,\tTraining Accuracy=0.0%\n",
      "iter  30:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter  40:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter  50:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter  60:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter  70:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter  80:\t Loss=3.02,\tTraining Accuracy=0.0%\n",
      "iter  90:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 100:\t Loss=3.62,\tTraining Accuracy=0.0%\n",
      "iter 110:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter 120:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 130:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 140:\t Loss=2.87,\tTraining Accuracy=100.0%\n",
      "iter 150:\t Loss=4.04,\tTraining Accuracy=0.0%\n",
      "iter 160:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 170:\t Loss=2.99,\tTraining Accuracy=0.0%\n",
      "iter 180:\t Loss=3.24,\tTraining Accuracy=100.0%\n",
      "iter 190:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 200:\t Loss=3.16,\tTraining Accuracy=0.0%\n",
      "iter 210:\t Loss=4.00,\tTraining Accuracy=0.0%\n",
      "iter 220:\t Loss=3.89,\tTraining Accuracy=0.0%\n",
      "iter 230:\t Loss=3.76,\tTraining Accuracy=0.0%\n",
      "iter 240:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 250:\t Loss=3.12,\tTraining Accuracy=0.0%\n",
      "iter 260:\t Loss=3.68,\tTraining Accuracy=0.0%\n",
      "iter 270:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 280:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 290:\t Loss=3.76,\tTraining Accuracy=0.0%\n",
      "iter 300:\t Loss=3.64,\tTraining Accuracy=0.0%\n",
      "iter 310:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 320:\t Loss=3.63,\tTraining Accuracy=0.0%\n",
      "iter 330:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 340:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 350:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 360:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 370:\t Loss=3.59,\tTraining Accuracy=0.0%\n",
      "iter 380:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 390:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 400:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 410:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 420:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter 430:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 440:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 450:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 460:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 470:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter 480:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 490:\t Loss=2.81,\tTraining Accuracy=0.0%\n",
      "iter 500:\t Loss=3.78,\tTraining Accuracy=0.0%\n",
      "iter 510:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter 520:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 530:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 540:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 550:\t Loss=3.57,\tTraining Accuracy=0.0%\n",
      "iter 560:\t Loss=3.32,\tTraining Accuracy=100.0%\n",
      "iter 570:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 580:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 590:\t Loss=2.89,\tTraining Accuracy=0.0%\n",
      "iter 600:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 610:\t Loss=3.05,\tTraining Accuracy=0.0%\n",
      "iter 620:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 630:\t Loss=3.00,\tTraining Accuracy=0.0%\n",
      "iter 640:\t Loss=3.77,\tTraining Accuracy=0.0%\n",
      "iter 650:\t Loss=3.58,\tTraining Accuracy=0.0%\n",
      "iter 660:\t Loss=2.70,\tTraining Accuracy=100.0%\n",
      "iter 670:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 680:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter 690:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 700:\t Loss=3.75,\tTraining Accuracy=0.0%\n",
      "iter 710:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 720:\t Loss=3.58,\tTraining Accuracy=0.0%\n",
      "iter 730:\t Loss=3.65,\tTraining Accuracy=0.0%\n",
      "iter 740:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 750:\t Loss=2.63,\tTraining Accuracy=0.0%\n",
      "iter 760:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 770:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 780:\t Loss=3.04,\tTraining Accuracy=0.0%\n",
      "iter 790:\t Loss=2.75,\tTraining Accuracy=100.0%\n",
      "iter 800:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 810:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 820:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter 830:\t Loss=3.67,\tTraining Accuracy=0.0%\n",
      "iter 840:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 850:\t Loss=3.73,\tTraining Accuracy=0.0%\n",
      "iter 860:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 870:\t Loss=3.04,\tTraining Accuracy=0.0%\n",
      "iter 880:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 890:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 900:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 910:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 920:\t Loss=3.86,\tTraining Accuracy=0.0%\n",
      "iter 930:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter 940:\t Loss=4.37,\tTraining Accuracy=0.0%\n",
      "iter 950:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 960:\t Loss=3.00,\tTraining Accuracy=0.0%\n",
      "iter 970:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 980:\t Loss=3.06,\tTraining Accuracy=0.0%\n",
      "iter 990:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 1000:\t Loss=2.96,\tTraining Accuracy=0.0%\n",
      "iter 1010:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 1020:\t Loss=3.68,\tTraining Accuracy=0.0%\n",
      "iter 1030:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 1040:\t Loss=3.07,\tTraining Accuracy=0.0%\n",
      "iter 1050:\t Loss=3.03,\tTraining Accuracy=0.0%\n",
      "iter 1060:\t Loss=2.55,\tTraining Accuracy=100.0%\n",
      "iter 1070:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 1080:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 1090:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 1100:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 1110:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter 1120:\t Loss=2.99,\tTraining Accuracy=0.0%\n",
      "iter 1130:\t Loss=3.66,\tTraining Accuracy=0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1140:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 1150:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 1160:\t Loss=3.74,\tTraining Accuracy=0.0%\n",
      "iter 1170:\t Loss=2.17,\tTraining Accuracy=100.0%\n",
      "iter 1180:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 1190:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 1200:\t Loss=3.69,\tTraining Accuracy=0.0%\n",
      "iter 1210:\t Loss=3.87,\tTraining Accuracy=0.0%\n",
      "iter 1220:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 1230:\t Loss=3.01,\tTraining Accuracy=0.0%\n",
      "iter 1240:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 1250:\t Loss=3.86,\tTraining Accuracy=0.0%\n",
      "iter 1260:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 1270:\t Loss=3.62,\tTraining Accuracy=0.0%\n",
      "iter 1280:\t Loss=2.37,\tTraining Accuracy=100.0%\n",
      "iter 1290:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 1300:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 1310:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 1320:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 1330:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 1340:\t Loss=3.04,\tTraining Accuracy=0.0%\n",
      "iter 1350:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 1360:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 1370:\t Loss=2.87,\tTraining Accuracy=100.0%\n",
      "iter 1380:\t Loss=2.91,\tTraining Accuracy=0.0%\n",
      "iter 1390:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 1400:\t Loss=2.94,\tTraining Accuracy=0.0%\n",
      "iter 1410:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 1420:\t Loss=2.89,\tTraining Accuracy=0.0%\n",
      "iter 1430:\t Loss=3.21,\tTraining Accuracy=0.0%\n",
      "iter 1440:\t Loss=3.07,\tTraining Accuracy=0.0%\n",
      "iter 1450:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 1460:\t Loss=3.20,\tTraining Accuracy=0.0%\n",
      "iter 1470:\t Loss=3.96,\tTraining Accuracy=0.0%\n",
      "iter 1480:\t Loss=2.83,\tTraining Accuracy=100.0%\n",
      "iter 1490:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 1500:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter 1510:\t Loss=2.96,\tTraining Accuracy=0.0%\n",
      "iter 1520:\t Loss=3.63,\tTraining Accuracy=0.0%\n",
      "iter 1530:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 1540:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 1550:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 1560:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 1570:\t Loss=3.06,\tTraining Accuracy=0.0%\n",
      "iter 1580:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 1590:\t Loss=2.36,\tTraining Accuracy=100.0%\n",
      "iter 1600:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter 1610:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter 1620:\t Loss=4.16,\tTraining Accuracy=0.0%\n",
      "iter 1630:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter 1640:\t Loss=2.92,\tTraining Accuracy=0.0%\n",
      "iter 1650:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 1660:\t Loss=3.94,\tTraining Accuracy=0.0%\n",
      "iter 1670:\t Loss=3.84,\tTraining Accuracy=0.0%\n",
      "iter 1680:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 1690:\t Loss=3.27,\tTraining Accuracy=100.0%\n",
      "iter 1700:\t Loss=2.50,\tTraining Accuracy=0.0%\n",
      "iter 1710:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 1720:\t Loss=2.71,\tTraining Accuracy=0.0%\n",
      "iter 1730:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter 1740:\t Loss=2.94,\tTraining Accuracy=0.0%\n",
      "iter 1750:\t Loss=3.83,\tTraining Accuracy=0.0%\n",
      "iter 1760:\t Loss=3.71,\tTraining Accuracy=0.0%\n",
      "iter 1770:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 1780:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 1790:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter 1800:\t Loss=2.90,\tTraining Accuracy=0.0%\n",
      "iter 1810:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 1820:\t Loss=3.17,\tTraining Accuracy=100.0%\n",
      "iter 1830:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 1840:\t Loss=3.58,\tTraining Accuracy=0.0%\n",
      "iter 1850:\t Loss=2.53,\tTraining Accuracy=100.0%\n",
      "iter 1860:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter 1870:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 1880:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 1890:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 1900:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 1910:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 1920:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 1930:\t Loss=3.76,\tTraining Accuracy=0.0%\n",
      "iter 1940:\t Loss=4.55,\tTraining Accuracy=0.0%\n",
      "iter 1950:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 1960:\t Loss=2.69,\tTraining Accuracy=0.0%\n",
      "iter 1970:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter 1980:\t Loss=3.57,\tTraining Accuracy=0.0%\n",
      "iter 1990:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 2000:\t Loss=2.95,\tTraining Accuracy=0.0%\n",
      "iter 2010:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 2020:\t Loss=2.92,\tTraining Accuracy=0.0%\n",
      "iter 2030:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 2040:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 2050:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 2060:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 2070:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 2080:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 2090:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 2100:\t Loss=3.20,\tTraining Accuracy=0.0%\n",
      "iter 2110:\t Loss=2.50,\tTraining Accuracy=100.0%\n",
      "iter 2120:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 2130:\t Loss=2.93,\tTraining Accuracy=0.0%\n",
      "iter 2140:\t Loss=3.56,\tTraining Accuracy=0.0%\n",
      "iter 2150:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 2160:\t Loss=3.04,\tTraining Accuracy=0.0%\n",
      "iter 2170:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 2180:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 2190:\t Loss=3.74,\tTraining Accuracy=0.0%\n",
      "iter 2200:\t Loss=4.11,\tTraining Accuracy=0.0%\n",
      "iter 2210:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 2220:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter 2230:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 2240:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 2250:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 2260:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 2270:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 2280:\t Loss=3.90,\tTraining Accuracy=0.0%\n",
      "iter 2290:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 2300:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 2310:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 2320:\t Loss=3.68,\tTraining Accuracy=0.0%\n",
      "iter 2330:\t Loss=3.33,\tTraining Accuracy=0.0%\n",
      "iter 2340:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 2350:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 2360:\t Loss=3.62,\tTraining Accuracy=0.0%\n",
      "iter 2370:\t Loss=3.25,\tTraining Accuracy=0.0%\n",
      "iter 2380:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 2390:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 2400:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 2410:\t Loss=3.21,\tTraining Accuracy=0.0%\n",
      "iter 2420:\t Loss=2.85,\tTraining Accuracy=0.0%\n",
      "iter 2430:\t Loss=2.77,\tTraining Accuracy=0.0%\n",
      "iter 2440:\t Loss=3.84,\tTraining Accuracy=0.0%\n",
      "iter 2450:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter 2460:\t Loss=2.72,\tTraining Accuracy=0.0%\n",
      "iter 2470:\t Loss=1.99,\tTraining Accuracy=100.0%\n",
      "iter 2480:\t Loss=3.72,\tTraining Accuracy=0.0%\n",
      "iter 2490:\t Loss=3.78,\tTraining Accuracy=0.0%\n",
      "iter 2500:\t Loss=2.84,\tTraining Accuracy=0.0%\n",
      "iter 2510:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter 2520:\t Loss=3.75,\tTraining Accuracy=0.0%\n",
      "iter 2530:\t Loss=3.33,\tTraining Accuracy=0.0%\n",
      "iter 2540:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 2550:\t Loss=3.07,\tTraining Accuracy=0.0%\n",
      "iter 2560:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter 2570:\t Loss=3.07,\tTraining Accuracy=0.0%\n",
      "iter 2580:\t Loss=2.79,\tTraining Accuracy=0.0%\n",
      "iter 2590:\t Loss=4.02,\tTraining Accuracy=0.0%\n",
      "iter 2600:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 2610:\t Loss=4.47,\tTraining Accuracy=0.0%\n",
      "iter 2620:\t Loss=2.73,\tTraining Accuracy=0.0%\n",
      "iter 2630:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter 2640:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 2650:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 2660:\t Loss=2.82,\tTraining Accuracy=0.0%\n",
      "iter 2670:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 2680:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter 2690:\t Loss=2.76,\tTraining Accuracy=0.0%\n",
      "iter 2700:\t Loss=3.73,\tTraining Accuracy=0.0%\n",
      "iter 2710:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter 2720:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 2730:\t Loss=3.20,\tTraining Accuracy=0.0%\n",
      "iter 2740:\t Loss=3.67,\tTraining Accuracy=0.0%\n",
      "iter 2750:\t Loss=2.73,\tTraining Accuracy=0.0%\n",
      "iter 2760:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter 2770:\t Loss=3.57,\tTraining Accuracy=0.0%\n",
      "iter 2780:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 2790:\t Loss=2.61,\tTraining Accuracy=100.0%\n",
      "iter 2800:\t Loss=2.85,\tTraining Accuracy=0.0%\n",
      "iter 2810:\t Loss=2.61,\tTraining Accuracy=0.0%\n",
      "iter 2820:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 2830:\t Loss=2.77,\tTraining Accuracy=100.0%\n",
      "iter 2840:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 2850:\t Loss=4.13,\tTraining Accuracy=0.0%\n",
      "iter 2860:\t Loss=2.92,\tTraining Accuracy=0.0%\n",
      "iter 2870:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter 2880:\t Loss=3.83,\tTraining Accuracy=0.0%\n",
      "iter 2890:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 2900:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 2910:\t Loss=2.81,\tTraining Accuracy=0.0%\n",
      "iter 2920:\t Loss=3.83,\tTraining Accuracy=0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2930:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 2940:\t Loss=2.93,\tTraining Accuracy=0.0%\n",
      "iter 2950:\t Loss=3.85,\tTraining Accuracy=0.0%\n",
      "iter 2960:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 2970:\t Loss=2.51,\tTraining Accuracy=0.0%\n",
      "iter 2980:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 2990:\t Loss=4.00,\tTraining Accuracy=0.0%\n",
      "iter 3000:\t Loss=2.55,\tTraining Accuracy=0.0%\n",
      "iter 3010:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 3020:\t Loss=3.66,\tTraining Accuracy=0.0%\n",
      "iter 3030:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 3040:\t Loss=2.72,\tTraining Accuracy=0.0%\n",
      "iter 3050:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 3060:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 3070:\t Loss=2.78,\tTraining Accuracy=0.0%\n",
      "iter 3080:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 3090:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter 3100:\t Loss=3.82,\tTraining Accuracy=0.0%\n",
      "iter 3110:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 3120:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 3130:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 3140:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter 3150:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 3160:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 3170:\t Loss=3.25,\tTraining Accuracy=0.0%\n",
      "iter 3180:\t Loss=2.71,\tTraining Accuracy=0.0%\n",
      "iter 3190:\t Loss=3.20,\tTraining Accuracy=0.0%\n",
      "iter 3200:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 3210:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 3220:\t Loss=3.05,\tTraining Accuracy=0.0%\n",
      "iter 3230:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 3240:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 3250:\t Loss=4.06,\tTraining Accuracy=0.0%\n",
      "iter 3260:\t Loss=3.65,\tTraining Accuracy=0.0%\n",
      "iter 3270:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 3280:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 3290:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 3300:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 3310:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 3320:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 3330:\t Loss=3.07,\tTraining Accuracy=0.0%\n",
      "iter 3340:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 3350:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 3360:\t Loss=2.48,\tTraining Accuracy=0.0%\n",
      "iter 3370:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter 3380:\t Loss=2.70,\tTraining Accuracy=0.0%\n",
      "iter 3390:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 3400:\t Loss=3.21,\tTraining Accuracy=100.0%\n",
      "iter 3410:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 3420:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 3430:\t Loss=2.04,\tTraining Accuracy=100.0%\n",
      "iter 3440:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 3450:\t Loss=2.80,\tTraining Accuracy=0.0%\n",
      "iter 3460:\t Loss=2.57,\tTraining Accuracy=0.0%\n",
      "iter 3470:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 3480:\t Loss=3.74,\tTraining Accuracy=0.0%\n",
      "iter 3490:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 3500:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 3510:\t Loss=3.57,\tTraining Accuracy=0.0%\n",
      "iter 3520:\t Loss=3.66,\tTraining Accuracy=0.0%\n",
      "iter 3530:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 3540:\t Loss=2.57,\tTraining Accuracy=0.0%\n",
      "iter 3550:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 3560:\t Loss=2.79,\tTraining Accuracy=0.0%\n",
      "iter 3570:\t Loss=2.96,\tTraining Accuracy=0.0%\n",
      "iter 3580:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 3590:\t Loss=3.78,\tTraining Accuracy=0.0%\n",
      "iter 3600:\t Loss=2.82,\tTraining Accuracy=100.0%\n",
      "iter 3610:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 3620:\t Loss=4.01,\tTraining Accuracy=0.0%\n",
      "iter 3630:\t Loss=2.69,\tTraining Accuracy=0.0%\n",
      "iter 3640:\t Loss=3.57,\tTraining Accuracy=0.0%\n",
      "iter 3650:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 3660:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 3670:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 3680:\t Loss=3.66,\tTraining Accuracy=0.0%\n",
      "iter 3690:\t Loss=4.94,\tTraining Accuracy=0.0%\n",
      "iter 3700:\t Loss=3.10,\tTraining Accuracy=0.0%\n",
      "iter 3710:\t Loss=2.94,\tTraining Accuracy=0.0%\n",
      "iter 3720:\t Loss=4.27,\tTraining Accuracy=0.0%\n",
      "iter 3730:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 3740:\t Loss=2.84,\tTraining Accuracy=0.0%\n",
      "iter 3750:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 3760:\t Loss=2.60,\tTraining Accuracy=0.0%\n",
      "iter 3770:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 3780:\t Loss=4.10,\tTraining Accuracy=0.0%\n",
      "iter 3790:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 3800:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 3810:\t Loss=3.84,\tTraining Accuracy=0.0%\n",
      "iter 3820:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 3830:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 3840:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 3850:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 3860:\t Loss=2.88,\tTraining Accuracy=0.0%\n",
      "iter 3870:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 3880:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 3890:\t Loss=3.95,\tTraining Accuracy=0.0%\n",
      "iter 3900:\t Loss=3.21,\tTraining Accuracy=0.0%\n",
      "iter 3910:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 3920:\t Loss=2.15,\tTraining Accuracy=100.0%\n",
      "iter 3930:\t Loss=3.03,\tTraining Accuracy=0.0%\n",
      "iter 3940:\t Loss=3.16,\tTraining Accuracy=0.0%\n",
      "iter 3950:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 3960:\t Loss=2.95,\tTraining Accuracy=100.0%\n",
      "iter 3970:\t Loss=2.09,\tTraining Accuracy=0.0%\n",
      "iter 3980:\t Loss=4.23,\tTraining Accuracy=0.0%\n",
      "iter 3990:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 4000:\t Loss=3.93,\tTraining Accuracy=0.0%\n",
      "iter 4010:\t Loss=3.21,\tTraining Accuracy=0.0%\n",
      "iter 4020:\t Loss=3.20,\tTraining Accuracy=0.0%\n",
      "iter 4030:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter 4040:\t Loss=3.67,\tTraining Accuracy=0.0%\n",
      "iter 4050:\t Loss=2.56,\tTraining Accuracy=100.0%\n",
      "iter 4060:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 4070:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 4080:\t Loss=3.02,\tTraining Accuracy=100.0%\n",
      "iter 4090:\t Loss=3.69,\tTraining Accuracy=0.0%\n",
      "iter 4100:\t Loss=3.06,\tTraining Accuracy=0.0%\n",
      "iter 4110:\t Loss=3.88,\tTraining Accuracy=0.0%\n",
      "iter 4120:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter 4130:\t Loss=5.31,\tTraining Accuracy=0.0%\n",
      "iter 4140:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 4150:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 4160:\t Loss=2.81,\tTraining Accuracy=0.0%\n",
      "iter 4170:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 4180:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 4190:\t Loss=4.19,\tTraining Accuracy=0.0%\n",
      "iter 4200:\t Loss=3.16,\tTraining Accuracy=0.0%\n",
      "iter 4210:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 4220:\t Loss=2.97,\tTraining Accuracy=0.0%\n",
      "iter 4230:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 4240:\t Loss=3.33,\tTraining Accuracy=0.0%\n",
      "iter 4250:\t Loss=4.36,\tTraining Accuracy=0.0%\n",
      "iter 4260:\t Loss=2.70,\tTraining Accuracy=0.0%\n",
      "iter 4270:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 4280:\t Loss=3.71,\tTraining Accuracy=0.0%\n",
      "iter 4290:\t Loss=2.90,\tTraining Accuracy=0.0%\n",
      "iter 4300:\t Loss=3.10,\tTraining Accuracy=0.0%\n",
      "iter 4310:\t Loss=5.22,\tTraining Accuracy=0.0%\n",
      "iter 4320:\t Loss=2.70,\tTraining Accuracy=0.0%\n",
      "iter 4330:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 4340:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 4350:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 4360:\t Loss=2.66,\tTraining Accuracy=0.0%\n",
      "iter 4370:\t Loss=2.99,\tTraining Accuracy=100.0%\n",
      "iter 4380:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 4390:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 4400:\t Loss=2.89,\tTraining Accuracy=0.0%\n",
      "iter 4410:\t Loss=3.82,\tTraining Accuracy=0.0%\n",
      "iter 4420:\t Loss=3.77,\tTraining Accuracy=0.0%\n",
      "iter 4430:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 4440:\t Loss=2.91,\tTraining Accuracy=0.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 2, validation loss: 3.31, validation accuracy: 10.0%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 3\n",
      "iter   0:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter  10:\t Loss=2.60,\tTraining Accuracy=0.0%\n",
      "iter  20:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter  30:\t Loss=3.59,\tTraining Accuracy=0.0%\n",
      "iter  40:\t Loss=3.59,\tTraining Accuracy=0.0%\n",
      "iter  50:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter  60:\t Loss=4.32,\tTraining Accuracy=0.0%\n",
      "iter  70:\t Loss=2.59,\tTraining Accuracy=0.0%\n",
      "iter  80:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter  90:\t Loss=5.37,\tTraining Accuracy=0.0%\n",
      "iter 100:\t Loss=2.69,\tTraining Accuracy=100.0%\n",
      "iter 110:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 120:\t Loss=3.17,\tTraining Accuracy=100.0%\n",
      "iter 130:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 140:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 150:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 160:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 170:\t Loss=3.82,\tTraining Accuracy=0.0%\n",
      "iter 180:\t Loss=3.64,\tTraining Accuracy=0.0%\n",
      "iter 190:\t Loss=3.06,\tTraining Accuracy=0.0%\n",
      "iter 200:\t Loss=3.94,\tTraining Accuracy=0.0%\n",
      "iter 210:\t Loss=3.69,\tTraining Accuracy=0.0%\n",
      "iter 220:\t Loss=3.10,\tTraining Accuracy=0.0%\n",
      "iter 230:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 240:\t Loss=3.69,\tTraining Accuracy=0.0%\n",
      "iter 250:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 260:\t Loss=2.93,\tTraining Accuracy=0.0%\n",
      "iter 270:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 280:\t Loss=3.67,\tTraining Accuracy=0.0%\n",
      "iter 290:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 300:\t Loss=3.21,\tTraining Accuracy=0.0%\n",
      "iter 310:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 320:\t Loss=2.93,\tTraining Accuracy=0.0%\n",
      "iter 330:\t Loss=2.31,\tTraining Accuracy=100.0%\n",
      "iter 340:\t Loss=3.24,\tTraining Accuracy=0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 350:\t Loss=3.98,\tTraining Accuracy=0.0%\n",
      "iter 360:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 370:\t Loss=3.95,\tTraining Accuracy=0.0%\n",
      "iter 380:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter 390:\t Loss=2.41,\tTraining Accuracy=100.0%\n",
      "iter 400:\t Loss=3.65,\tTraining Accuracy=0.0%\n",
      "iter 410:\t Loss=2.92,\tTraining Accuracy=0.0%\n",
      "iter 420:\t Loss=3.21,\tTraining Accuracy=0.0%\n",
      "iter 430:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 440:\t Loss=2.08,\tTraining Accuracy=100.0%\n",
      "iter 450:\t Loss=2.58,\tTraining Accuracy=0.0%\n",
      "iter 460:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 470:\t Loss=2.84,\tTraining Accuracy=0.0%\n",
      "iter 480:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 490:\t Loss=2.83,\tTraining Accuracy=0.0%\n",
      "iter 500:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 510:\t Loss=2.92,\tTraining Accuracy=100.0%\n",
      "iter 520:\t Loss=2.96,\tTraining Accuracy=0.0%\n",
      "iter 530:\t Loss=2.70,\tTraining Accuracy=0.0%\n",
      "iter 540:\t Loss=3.59,\tTraining Accuracy=0.0%\n",
      "iter 550:\t Loss=2.97,\tTraining Accuracy=0.0%\n",
      "iter 560:\t Loss=2.55,\tTraining Accuracy=100.0%\n",
      "iter 570:\t Loss=2.77,\tTraining Accuracy=100.0%\n",
      "iter 580:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 590:\t Loss=3.06,\tTraining Accuracy=0.0%\n",
      "iter 600:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 610:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter 620:\t Loss=1.96,\tTraining Accuracy=100.0%\n",
      "iter 630:\t Loss=3.78,\tTraining Accuracy=0.0%\n",
      "iter 640:\t Loss=3.68,\tTraining Accuracy=0.0%\n",
      "iter 650:\t Loss=3.69,\tTraining Accuracy=0.0%\n",
      "iter 660:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 670:\t Loss=2.33,\tTraining Accuracy=0.0%\n",
      "iter 680:\t Loss=2.89,\tTraining Accuracy=100.0%\n",
      "iter 690:\t Loss=2.93,\tTraining Accuracy=100.0%\n",
      "iter 700:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 710:\t Loss=2.58,\tTraining Accuracy=0.0%\n",
      "iter 720:\t Loss=3.21,\tTraining Accuracy=0.0%\n",
      "iter 730:\t Loss=3.95,\tTraining Accuracy=0.0%\n",
      "iter 740:\t Loss=2.90,\tTraining Accuracy=100.0%\n",
      "iter 750:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 760:\t Loss=3.33,\tTraining Accuracy=0.0%\n",
      "iter 770:\t Loss=3.03,\tTraining Accuracy=0.0%\n",
      "iter 780:\t Loss=2.90,\tTraining Accuracy=0.0%\n",
      "iter 790:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 800:\t Loss=3.33,\tTraining Accuracy=0.0%\n",
      "iter 810:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 820:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 830:\t Loss=3.33,\tTraining Accuracy=0.0%\n",
      "iter 840:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 850:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter 860:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 870:\t Loss=3.62,\tTraining Accuracy=0.0%\n",
      "iter 880:\t Loss=3.25,\tTraining Accuracy=0.0%\n",
      "iter 890:\t Loss=2.88,\tTraining Accuracy=0.0%\n",
      "iter 900:\t Loss=3.02,\tTraining Accuracy=0.0%\n",
      "iter 910:\t Loss=3.56,\tTraining Accuracy=0.0%\n",
      "iter 920:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 930:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 940:\t Loss=3.70,\tTraining Accuracy=0.0%\n",
      "iter 950:\t Loss=2.81,\tTraining Accuracy=100.0%\n",
      "iter 960:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 970:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 980:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 990:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 1000:\t Loss=3.00,\tTraining Accuracy=0.0%\n",
      "iter 1010:\t Loss=3.72,\tTraining Accuracy=0.0%\n",
      "iter 1020:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 1030:\t Loss=3.69,\tTraining Accuracy=0.0%\n",
      "iter 1040:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 1050:\t Loss=2.20,\tTraining Accuracy=0.0%\n",
      "iter 1060:\t Loss=3.67,\tTraining Accuracy=0.0%\n",
      "iter 1070:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 1080:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter 1090:\t Loss=2.64,\tTraining Accuracy=100.0%\n",
      "iter 1100:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 1110:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 1120:\t Loss=2.86,\tTraining Accuracy=0.0%\n",
      "iter 1130:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 1140:\t Loss=3.20,\tTraining Accuracy=0.0%\n",
      "iter 1150:\t Loss=4.66,\tTraining Accuracy=0.0%\n",
      "iter 1160:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 1170:\t Loss=2.83,\tTraining Accuracy=100.0%\n",
      "iter 1180:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 1190:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter 1200:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 1210:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 1220:\t Loss=5.96,\tTraining Accuracy=0.0%\n",
      "iter 1230:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 1240:\t Loss=2.84,\tTraining Accuracy=0.0%\n",
      "iter 1250:\t Loss=2.87,\tTraining Accuracy=0.0%\n",
      "iter 1260:\t Loss=1.97,\tTraining Accuracy=100.0%\n",
      "iter 1270:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 1280:\t Loss=2.98,\tTraining Accuracy=0.0%\n",
      "iter 1290:\t Loss=2.79,\tTraining Accuracy=0.0%\n",
      "iter 1300:\t Loss=2.17,\tTraining Accuracy=100.0%\n",
      "iter 1310:\t Loss=4.61,\tTraining Accuracy=0.0%\n",
      "iter 1320:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 1330:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 1340:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter 1350:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 1360:\t Loss=2.63,\tTraining Accuracy=0.0%\n",
      "iter 1370:\t Loss=2.91,\tTraining Accuracy=0.0%\n",
      "iter 1380:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 1390:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter 1400:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 1410:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 1420:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 1430:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 1440:\t Loss=2.74,\tTraining Accuracy=0.0%\n",
      "iter 1450:\t Loss=2.96,\tTraining Accuracy=0.0%\n",
      "iter 1460:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 1470:\t Loss=3.10,\tTraining Accuracy=0.0%\n",
      "iter 1480:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 1490:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter 1500:\t Loss=2.81,\tTraining Accuracy=0.0%\n",
      "iter 1510:\t Loss=2.96,\tTraining Accuracy=0.0%\n",
      "iter 1520:\t Loss=2.65,\tTraining Accuracy=100.0%\n",
      "iter 1530:\t Loss=3.65,\tTraining Accuracy=0.0%\n",
      "iter 1540:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 1550:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 1560:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter 1570:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 1580:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter 1590:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 1600:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 1610:\t Loss=4.11,\tTraining Accuracy=0.0%\n",
      "iter 1620:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 1630:\t Loss=3.98,\tTraining Accuracy=0.0%\n",
      "iter 1640:\t Loss=3.13,\tTraining Accuracy=100.0%\n",
      "iter 1650:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 1660:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 1670:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 1680:\t Loss=2.86,\tTraining Accuracy=0.0%\n",
      "iter 1690:\t Loss=3.20,\tTraining Accuracy=0.0%\n",
      "iter 1700:\t Loss=5.15,\tTraining Accuracy=0.0%\n",
      "iter 1710:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 1720:\t Loss=2.64,\tTraining Accuracy=0.0%\n",
      "iter 1730:\t Loss=2.71,\tTraining Accuracy=0.0%\n",
      "iter 1740:\t Loss=2.91,\tTraining Accuracy=0.0%\n",
      "iter 1750:\t Loss=3.81,\tTraining Accuracy=0.0%\n",
      "iter 1760:\t Loss=2.10,\tTraining Accuracy=100.0%\n",
      "iter 1770:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter 1780:\t Loss=3.81,\tTraining Accuracy=0.0%\n",
      "iter 1790:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 1800:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 1810:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 1820:\t Loss=2.61,\tTraining Accuracy=0.0%\n",
      "iter 1830:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 1840:\t Loss=2.47,\tTraining Accuracy=0.0%\n",
      "iter 1850:\t Loss=2.95,\tTraining Accuracy=0.0%\n",
      "iter 1860:\t Loss=2.95,\tTraining Accuracy=0.0%\n",
      "iter 1870:\t Loss=2.84,\tTraining Accuracy=0.0%\n",
      "iter 1880:\t Loss=2.74,\tTraining Accuracy=0.0%\n",
      "iter 1890:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 1900:\t Loss=4.40,\tTraining Accuracy=0.0%\n",
      "iter 1910:\t Loss=2.08,\tTraining Accuracy=100.0%\n",
      "iter 1920:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 1930:\t Loss=2.38,\tTraining Accuracy=0.0%\n",
      "iter 1940:\t Loss=2.20,\tTraining Accuracy=0.0%\n",
      "iter 1950:\t Loss=2.55,\tTraining Accuracy=100.0%\n",
      "iter 1960:\t Loss=3.65,\tTraining Accuracy=0.0%\n",
      "iter 1970:\t Loss=4.36,\tTraining Accuracy=0.0%\n",
      "iter 1980:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 1990:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 2000:\t Loss=2.74,\tTraining Accuracy=0.0%\n",
      "iter 2010:\t Loss=2.44,\tTraining Accuracy=0.0%\n",
      "iter 2020:\t Loss=3.74,\tTraining Accuracy=0.0%\n",
      "iter 2030:\t Loss=2.97,\tTraining Accuracy=0.0%\n",
      "iter 2040:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 2050:\t Loss=3.01,\tTraining Accuracy=0.0%\n",
      "iter 2060:\t Loss=2.84,\tTraining Accuracy=0.0%\n",
      "iter 2070:\t Loss=4.33,\tTraining Accuracy=0.0%\n",
      "iter 2080:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 2090:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 2100:\t Loss=3.56,\tTraining Accuracy=0.0%\n",
      "iter 2110:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 2120:\t Loss=2.46,\tTraining Accuracy=0.0%\n",
      "iter 2130:\t Loss=2.92,\tTraining Accuracy=0.0%\n",
      "iter 2140:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 2150:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 2160:\t Loss=2.77,\tTraining Accuracy=0.0%\n",
      "iter 2170:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 2180:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 2190:\t Loss=2.89,\tTraining Accuracy=100.0%\n",
      "iter 2200:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter 2210:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 2220:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 2230:\t Loss=3.02,\tTraining Accuracy=0.0%\n",
      "iter 2240:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 2250:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 2260:\t Loss=3.59,\tTraining Accuracy=0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2270:\t Loss=4.00,\tTraining Accuracy=0.0%\n",
      "iter 2280:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 2290:\t Loss=2.94,\tTraining Accuracy=0.0%\n",
      "iter 2300:\t Loss=2.08,\tTraining Accuracy=100.0%\n",
      "iter 2310:\t Loss=3.93,\tTraining Accuracy=0.0%\n",
      "iter 2320:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 2330:\t Loss=3.99,\tTraining Accuracy=0.0%\n",
      "iter 2340:\t Loss=4.46,\tTraining Accuracy=0.0%\n",
      "iter 2350:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 2360:\t Loss=3.16,\tTraining Accuracy=0.0%\n",
      "iter 2370:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 2380:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter 2390:\t Loss=3.10,\tTraining Accuracy=0.0%\n",
      "iter 2400:\t Loss=3.03,\tTraining Accuracy=0.0%\n",
      "iter 2410:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 2420:\t Loss=3.59,\tTraining Accuracy=0.0%\n",
      "iter 2430:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 2440:\t Loss=3.79,\tTraining Accuracy=0.0%\n",
      "iter 2450:\t Loss=3.01,\tTraining Accuracy=100.0%\n",
      "iter 2460:\t Loss=3.62,\tTraining Accuracy=0.0%\n",
      "iter 2470:\t Loss=2.71,\tTraining Accuracy=0.0%\n",
      "iter 2480:\t Loss=2.49,\tTraining Accuracy=100.0%\n",
      "iter 2490:\t Loss=4.13,\tTraining Accuracy=0.0%\n",
      "iter 2500:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 2510:\t Loss=2.23,\tTraining Accuracy=0.0%\n",
      "iter 2520:\t Loss=2.72,\tTraining Accuracy=100.0%\n",
      "iter 2530:\t Loss=2.17,\tTraining Accuracy=100.0%\n",
      "iter 2540:\t Loss=2.78,\tTraining Accuracy=0.0%\n",
      "iter 2550:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 2560:\t Loss=3.57,\tTraining Accuracy=0.0%\n",
      "iter 2570:\t Loss=2.84,\tTraining Accuracy=0.0%\n",
      "iter 2580:\t Loss=3.01,\tTraining Accuracy=0.0%\n",
      "iter 2590:\t Loss=2.77,\tTraining Accuracy=100.0%\n",
      "iter 2600:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter 2610:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 2620:\t Loss=3.16,\tTraining Accuracy=0.0%\n",
      "iter 2630:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 2640:\t Loss=3.65,\tTraining Accuracy=0.0%\n",
      "iter 2650:\t Loss=2.73,\tTraining Accuracy=0.0%\n",
      "iter 2660:\t Loss=3.12,\tTraining Accuracy=0.0%\n",
      "iter 2670:\t Loss=3.05,\tTraining Accuracy=0.0%\n",
      "iter 2680:\t Loss=3.59,\tTraining Accuracy=0.0%\n",
      "iter 2690:\t Loss=3.64,\tTraining Accuracy=0.0%\n",
      "iter 2700:\t Loss=4.41,\tTraining Accuracy=0.0%\n",
      "iter 2710:\t Loss=2.37,\tTraining Accuracy=0.0%\n",
      "iter 2720:\t Loss=2.60,\tTraining Accuracy=0.0%\n",
      "iter 2730:\t Loss=4.54,\tTraining Accuracy=0.0%\n",
      "iter 2740:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 2750:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 2760:\t Loss=2.51,\tTraining Accuracy=0.0%\n",
      "iter 2770:\t Loss=3.12,\tTraining Accuracy=0.0%\n",
      "iter 2780:\t Loss=3.89,\tTraining Accuracy=0.0%\n",
      "iter 2790:\t Loss=2.19,\tTraining Accuracy=100.0%\n",
      "iter 2800:\t Loss=3.63,\tTraining Accuracy=0.0%\n",
      "iter 2810:\t Loss=3.06,\tTraining Accuracy=0.0%\n",
      "iter 2820:\t Loss=3.05,\tTraining Accuracy=0.0%\n",
      "iter 2830:\t Loss=3.81,\tTraining Accuracy=0.0%\n",
      "iter 2840:\t Loss=2.90,\tTraining Accuracy=0.0%\n",
      "iter 2850:\t Loss=3.33,\tTraining Accuracy=0.0%\n",
      "iter 2860:\t Loss=2.80,\tTraining Accuracy=100.0%\n",
      "iter 2870:\t Loss=3.70,\tTraining Accuracy=0.0%\n",
      "iter 2880:\t Loss=3.07,\tTraining Accuracy=0.0%\n",
      "iter 2890:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 2900:\t Loss=4.06,\tTraining Accuracy=0.0%\n",
      "iter 2910:\t Loss=3.05,\tTraining Accuracy=100.0%\n",
      "iter 2920:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 2930:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 2940:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter 2950:\t Loss=3.01,\tTraining Accuracy=0.0%\n",
      "iter 2960:\t Loss=4.34,\tTraining Accuracy=0.0%\n",
      "iter 2970:\t Loss=3.12,\tTraining Accuracy=0.0%\n",
      "iter 2980:\t Loss=2.47,\tTraining Accuracy=0.0%\n",
      "iter 2990:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 3000:\t Loss=2.43,\tTraining Accuracy=0.0%\n",
      "iter 3010:\t Loss=2.01,\tTraining Accuracy=0.0%\n",
      "iter 3020:\t Loss=2.91,\tTraining Accuracy=0.0%\n",
      "iter 3030:\t Loss=3.63,\tTraining Accuracy=0.0%\n",
      "iter 3040:\t Loss=2.51,\tTraining Accuracy=0.0%\n",
      "iter 3050:\t Loss=3.85,\tTraining Accuracy=0.0%\n",
      "iter 3060:\t Loss=4.07,\tTraining Accuracy=0.0%\n",
      "iter 3070:\t Loss=2.85,\tTraining Accuracy=0.0%\n",
      "iter 3080:\t Loss=2.85,\tTraining Accuracy=100.0%\n",
      "iter 3090:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 3100:\t Loss=3.79,\tTraining Accuracy=0.0%\n",
      "iter 3110:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 3120:\t Loss=3.05,\tTraining Accuracy=0.0%\n",
      "iter 3130:\t Loss=2.83,\tTraining Accuracy=0.0%\n",
      "iter 3140:\t Loss=3.06,\tTraining Accuracy=0.0%\n",
      "iter 3150:\t Loss=4.24,\tTraining Accuracy=0.0%\n",
      "iter 3160:\t Loss=3.06,\tTraining Accuracy=0.0%\n",
      "iter 3170:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 3180:\t Loss=2.93,\tTraining Accuracy=0.0%\n",
      "iter 3190:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 3200:\t Loss=2.72,\tTraining Accuracy=0.0%\n",
      "iter 3210:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 3220:\t Loss=3.10,\tTraining Accuracy=0.0%\n",
      "iter 3230:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 3240:\t Loss=1.79,\tTraining Accuracy=100.0%\n",
      "iter 3250:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 3260:\t Loss=3.17,\tTraining Accuracy=100.0%\n",
      "iter 3270:\t Loss=5.05,\tTraining Accuracy=0.0%\n",
      "iter 3280:\t Loss=2.67,\tTraining Accuracy=0.0%\n",
      "iter 3290:\t Loss=3.92,\tTraining Accuracy=0.0%\n",
      "iter 3300:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 3310:\t Loss=2.78,\tTraining Accuracy=0.0%\n",
      "iter 3320:\t Loss=2.79,\tTraining Accuracy=0.0%\n",
      "iter 3330:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 3340:\t Loss=3.10,\tTraining Accuracy=0.0%\n",
      "iter 3350:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 3360:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 3370:\t Loss=3.07,\tTraining Accuracy=0.0%\n",
      "iter 3380:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 3390:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 3400:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 3410:\t Loss=3.96,\tTraining Accuracy=0.0%\n",
      "iter 3420:\t Loss=2.64,\tTraining Accuracy=0.0%\n",
      "iter 3430:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 3440:\t Loss=2.11,\tTraining Accuracy=100.0%\n",
      "iter 3450:\t Loss=3.89,\tTraining Accuracy=0.0%\n",
      "iter 3460:\t Loss=3.03,\tTraining Accuracy=0.0%\n",
      "iter 3470:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 3480:\t Loss=3.02,\tTraining Accuracy=0.0%\n",
      "iter 3490:\t Loss=2.43,\tTraining Accuracy=0.0%\n",
      "iter 3500:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 3510:\t Loss=3.74,\tTraining Accuracy=0.0%\n",
      "iter 3520:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter 3530:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 3540:\t Loss=3.05,\tTraining Accuracy=0.0%\n",
      "iter 3550:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 3560:\t Loss=2.45,\tTraining Accuracy=0.0%\n",
      "iter 3570:\t Loss=3.77,\tTraining Accuracy=0.0%\n",
      "iter 3580:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 3590:\t Loss=2.26,\tTraining Accuracy=0.0%\n",
      "iter 3600:\t Loss=3.05,\tTraining Accuracy=100.0%\n",
      "iter 3610:\t Loss=2.45,\tTraining Accuracy=0.0%\n",
      "iter 3620:\t Loss=2.98,\tTraining Accuracy=0.0%\n",
      "iter 3630:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 3640:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 3650:\t Loss=2.40,\tTraining Accuracy=100.0%\n",
      "iter 3660:\t Loss=2.82,\tTraining Accuracy=0.0%\n",
      "iter 3670:\t Loss=2.70,\tTraining Accuracy=0.0%\n",
      "iter 3680:\t Loss=2.78,\tTraining Accuracy=100.0%\n",
      "iter 3690:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 3700:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 3710:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 3720:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 3730:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 3740:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 3750:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 3760:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 3770:\t Loss=2.90,\tTraining Accuracy=0.0%\n",
      "iter 3780:\t Loss=2.94,\tTraining Accuracy=0.0%\n",
      "iter 3790:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 3800:\t Loss=3.92,\tTraining Accuracy=0.0%\n",
      "iter 3810:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 3820:\t Loss=3.25,\tTraining Accuracy=0.0%\n",
      "iter 3830:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 3840:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 3850:\t Loss=4.01,\tTraining Accuracy=0.0%\n",
      "iter 3860:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 3870:\t Loss=2.58,\tTraining Accuracy=0.0%\n",
      "iter 3880:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter 3890:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter 3900:\t Loss=4.42,\tTraining Accuracy=0.0%\n",
      "iter 3910:\t Loss=2.85,\tTraining Accuracy=0.0%\n",
      "iter 3920:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 3930:\t Loss=2.57,\tTraining Accuracy=0.0%\n",
      "iter 3940:\t Loss=2.86,\tTraining Accuracy=0.0%\n",
      "iter 3950:\t Loss=4.15,\tTraining Accuracy=0.0%\n",
      "iter 3960:\t Loss=2.49,\tTraining Accuracy=100.0%\n",
      "iter 3970:\t Loss=2.96,\tTraining Accuracy=0.0%\n",
      "iter 3980:\t Loss=3.12,\tTraining Accuracy=0.0%\n",
      "iter 3990:\t Loss=3.58,\tTraining Accuracy=0.0%\n",
      "iter 4000:\t Loss=3.06,\tTraining Accuracy=0.0%\n",
      "iter 4010:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 4020:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 4030:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 4040:\t Loss=2.97,\tTraining Accuracy=0.0%\n",
      "iter 4050:\t Loss=3.85,\tTraining Accuracy=0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4060:\t Loss=3.06,\tTraining Accuracy=0.0%\n",
      "iter 4070:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 4080:\t Loss=2.48,\tTraining Accuracy=0.0%\n",
      "iter 4090:\t Loss=2.16,\tTraining Accuracy=100.0%\n",
      "iter 4100:\t Loss=4.02,\tTraining Accuracy=0.0%\n",
      "iter 4110:\t Loss=2.78,\tTraining Accuracy=0.0%\n",
      "iter 4120:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 4130:\t Loss=3.25,\tTraining Accuracy=0.0%\n",
      "iter 4140:\t Loss=1.91,\tTraining Accuracy=0.0%\n",
      "iter 4150:\t Loss=2.73,\tTraining Accuracy=100.0%\n",
      "iter 4160:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter 4170:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 4180:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 4190:\t Loss=4.11,\tTraining Accuracy=0.0%\n",
      "iter 4200:\t Loss=2.88,\tTraining Accuracy=100.0%\n",
      "iter 4210:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 4220:\t Loss=2.62,\tTraining Accuracy=0.0%\n",
      "iter 4230:\t Loss=3.33,\tTraining Accuracy=0.0%\n",
      "iter 4240:\t Loss=3.68,\tTraining Accuracy=0.0%\n",
      "iter 4250:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 4260:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 4270:\t Loss=2.21,\tTraining Accuracy=100.0%\n",
      "iter 4280:\t Loss=3.16,\tTraining Accuracy=0.0%\n",
      "iter 4290:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 4300:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 4310:\t Loss=2.81,\tTraining Accuracy=0.0%\n",
      "iter 4320:\t Loss=3.62,\tTraining Accuracy=0.0%\n",
      "iter 4330:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 4340:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter 4350:\t Loss=2.97,\tTraining Accuracy=0.0%\n",
      "iter 4360:\t Loss=1.97,\tTraining Accuracy=0.0%\n",
      "iter 4370:\t Loss=3.12,\tTraining Accuracy=0.0%\n",
      "iter 4380:\t Loss=2.81,\tTraining Accuracy=0.0%\n",
      "iter 4390:\t Loss=2.55,\tTraining Accuracy=0.0%\n",
      "iter 4400:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 4410:\t Loss=4.73,\tTraining Accuracy=0.0%\n",
      "iter 4420:\t Loss=1.83,\tTraining Accuracy=100.0%\n",
      "iter 4430:\t Loss=1.53,\tTraining Accuracy=100.0%\n",
      "iter 4440:\t Loss=2.62,\tTraining Accuracy=0.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 3, validation loss: 3.27, validation accuracy: 10.0%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 4\n",
      "iter   0:\t Loss=2.26,\tTraining Accuracy=0.0%\n",
      "iter  10:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter  20:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter  30:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter  40:\t Loss=2.41,\tTraining Accuracy=0.0%\n",
      "iter  50:\t Loss=2.77,\tTraining Accuracy=0.0%\n",
      "iter  60:\t Loss=2.95,\tTraining Accuracy=0.0%\n",
      "iter  70:\t Loss=2.87,\tTraining Accuracy=0.0%\n",
      "iter  80:\t Loss=3.21,\tTraining Accuracy=0.0%\n",
      "iter  90:\t Loss=2.85,\tTraining Accuracy=0.0%\n",
      "iter 100:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 110:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 120:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 130:\t Loss=2.58,\tTraining Accuracy=0.0%\n",
      "iter 140:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 150:\t Loss=2.73,\tTraining Accuracy=100.0%\n",
      "iter 160:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 170:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter 180:\t Loss=2.59,\tTraining Accuracy=0.0%\n",
      "iter 190:\t Loss=2.36,\tTraining Accuracy=0.0%\n",
      "iter 200:\t Loss=6.85,\tTraining Accuracy=0.0%\n",
      "iter 210:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter 220:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 230:\t Loss=2.95,\tTraining Accuracy=0.0%\n",
      "iter 240:\t Loss=2.93,\tTraining Accuracy=0.0%\n",
      "iter 250:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 260:\t Loss=2.65,\tTraining Accuracy=0.0%\n",
      "iter 270:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter 280:\t Loss=2.41,\tTraining Accuracy=0.0%\n",
      "iter 290:\t Loss=2.81,\tTraining Accuracy=100.0%\n",
      "iter 300:\t Loss=2.24,\tTraining Accuracy=100.0%\n",
      "iter 310:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 320:\t Loss=2.34,\tTraining Accuracy=0.0%\n",
      "iter 330:\t Loss=1.86,\tTraining Accuracy=100.0%\n",
      "iter 340:\t Loss=2.77,\tTraining Accuracy=0.0%\n",
      "iter 350:\t Loss=5.88,\tTraining Accuracy=0.0%\n",
      "iter 360:\t Loss=2.34,\tTraining Accuracy=0.0%\n",
      "iter 370:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 380:\t Loss=3.00,\tTraining Accuracy=100.0%\n",
      "iter 390:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 400:\t Loss=4.84,\tTraining Accuracy=0.0%\n",
      "iter 410:\t Loss=2.79,\tTraining Accuracy=0.0%\n",
      "iter 420:\t Loss=3.78,\tTraining Accuracy=0.0%\n",
      "iter 430:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 440:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 450:\t Loss=4.01,\tTraining Accuracy=0.0%\n",
      "iter 460:\t Loss=3.07,\tTraining Accuracy=0.0%\n",
      "iter 470:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 480:\t Loss=2.96,\tTraining Accuracy=0.0%\n",
      "iter 490:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 500:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 510:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 520:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 530:\t Loss=2.61,\tTraining Accuracy=0.0%\n",
      "iter 540:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 550:\t Loss=2.41,\tTraining Accuracy=0.0%\n",
      "iter 560:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 570:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 580:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 590:\t Loss=2.92,\tTraining Accuracy=0.0%\n",
      "iter 600:\t Loss=2.63,\tTraining Accuracy=0.0%\n",
      "iter 610:\t Loss=2.70,\tTraining Accuracy=0.0%\n",
      "iter 620:\t Loss=2.68,\tTraining Accuracy=0.0%\n",
      "iter 630:\t Loss=2.99,\tTraining Accuracy=0.0%\n",
      "iter 640:\t Loss=4.30,\tTraining Accuracy=0.0%\n",
      "iter 650:\t Loss=2.26,\tTraining Accuracy=100.0%\n",
      "iter 660:\t Loss=2.61,\tTraining Accuracy=100.0%\n",
      "iter 670:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter 680:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 690:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter 700:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 710:\t Loss=3.00,\tTraining Accuracy=0.0%\n",
      "iter 720:\t Loss=3.00,\tTraining Accuracy=100.0%\n",
      "iter 730:\t Loss=2.05,\tTraining Accuracy=0.0%\n",
      "iter 740:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 750:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 760:\t Loss=2.80,\tTraining Accuracy=0.0%\n",
      "iter 770:\t Loss=2.18,\tTraining Accuracy=0.0%\n",
      "iter 780:\t Loss=3.33,\tTraining Accuracy=0.0%\n",
      "iter 790:\t Loss=2.33,\tTraining Accuracy=0.0%\n",
      "iter 800:\t Loss=2.89,\tTraining Accuracy=0.0%\n",
      "iter 810:\t Loss=1.94,\tTraining Accuracy=100.0%\n",
      "iter 820:\t Loss=3.65,\tTraining Accuracy=0.0%\n",
      "iter 830:\t Loss=3.00,\tTraining Accuracy=0.0%\n",
      "iter 840:\t Loss=4.87,\tTraining Accuracy=0.0%\n",
      "iter 850:\t Loss=2.28,\tTraining Accuracy=0.0%\n",
      "iter 860:\t Loss=3.33,\tTraining Accuracy=0.0%\n",
      "iter 870:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 880:\t Loss=2.47,\tTraining Accuracy=0.0%\n",
      "iter 890:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 900:\t Loss=3.04,\tTraining Accuracy=0.0%\n",
      "iter 910:\t Loss=3.02,\tTraining Accuracy=0.0%\n",
      "iter 920:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 930:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 940:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 950:\t Loss=4.07,\tTraining Accuracy=0.0%\n",
      "iter 960:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 970:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 980:\t Loss=2.84,\tTraining Accuracy=100.0%\n",
      "iter 990:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 1000:\t Loss=2.79,\tTraining Accuracy=100.0%\n",
      "iter 1010:\t Loss=2.71,\tTraining Accuracy=0.0%\n",
      "iter 1020:\t Loss=3.59,\tTraining Accuracy=0.0%\n",
      "iter 1030:\t Loss=3.12,\tTraining Accuracy=0.0%\n",
      "iter 1040:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter 1050:\t Loss=3.25,\tTraining Accuracy=0.0%\n",
      "iter 1060:\t Loss=2.39,\tTraining Accuracy=0.0%\n",
      "iter 1070:\t Loss=2.10,\tTraining Accuracy=0.0%\n",
      "iter 1080:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 1090:\t Loss=2.14,\tTraining Accuracy=0.0%\n",
      "iter 1100:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 1110:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 1120:\t Loss=2.23,\tTraining Accuracy=0.0%\n",
      "iter 1130:\t Loss=3.93,\tTraining Accuracy=0.0%\n",
      "iter 1140:\t Loss=2.57,\tTraining Accuracy=100.0%\n",
      "iter 1150:\t Loss=2.51,\tTraining Accuracy=0.0%\n",
      "iter 1160:\t Loss=3.21,\tTraining Accuracy=0.0%\n",
      "iter 1170:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 1180:\t Loss=3.93,\tTraining Accuracy=0.0%\n",
      "iter 1190:\t Loss=2.83,\tTraining Accuracy=0.0%\n",
      "iter 1200:\t Loss=2.79,\tTraining Accuracy=0.0%\n",
      "iter 1210:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 1220:\t Loss=5.23,\tTraining Accuracy=0.0%\n",
      "iter 1230:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 1240:\t Loss=2.88,\tTraining Accuracy=0.0%\n",
      "iter 1250:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 1260:\t Loss=3.70,\tTraining Accuracy=0.0%\n",
      "iter 1270:\t Loss=3.75,\tTraining Accuracy=0.0%\n",
      "iter 1280:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 1290:\t Loss=2.82,\tTraining Accuracy=0.0%\n",
      "iter 1300:\t Loss=2.90,\tTraining Accuracy=0.0%\n",
      "iter 1310:\t Loss=2.96,\tTraining Accuracy=100.0%\n",
      "iter 1320:\t Loss=2.36,\tTraining Accuracy=100.0%\n",
      "iter 1330:\t Loss=2.75,\tTraining Accuracy=0.0%\n",
      "iter 1340:\t Loss=2.00,\tTraining Accuracy=100.0%\n",
      "iter 1350:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter 1360:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 1370:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 1380:\t Loss=2.92,\tTraining Accuracy=0.0%\n",
      "iter 1390:\t Loss=3.23,\tTraining Accuracy=0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1400:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 1410:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 1420:\t Loss=3.25,\tTraining Accuracy=0.0%\n",
      "iter 1430:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 1440:\t Loss=3.02,\tTraining Accuracy=0.0%\n",
      "iter 1450:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 1460:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 1470:\t Loss=1.86,\tTraining Accuracy=0.0%\n",
      "iter 1480:\t Loss=3.10,\tTraining Accuracy=0.0%\n",
      "iter 1490:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 1500:\t Loss=2.57,\tTraining Accuracy=100.0%\n",
      "iter 1510:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 1520:\t Loss=3.02,\tTraining Accuracy=0.0%\n",
      "iter 1530:\t Loss=2.19,\tTraining Accuracy=0.0%\n",
      "iter 1540:\t Loss=3.20,\tTraining Accuracy=0.0%\n",
      "iter 1550:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 1560:\t Loss=2.36,\tTraining Accuracy=0.0%\n",
      "iter 1570:\t Loss=3.80,\tTraining Accuracy=0.0%\n",
      "iter 1580:\t Loss=4.97,\tTraining Accuracy=0.0%\n",
      "iter 1590:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 1600:\t Loss=3.65,\tTraining Accuracy=0.0%\n",
      "iter 1610:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 1620:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter 1630:\t Loss=3.02,\tTraining Accuracy=0.0%\n",
      "iter 1640:\t Loss=2.79,\tTraining Accuracy=0.0%\n",
      "iter 1650:\t Loss=2.92,\tTraining Accuracy=0.0%\n",
      "iter 1660:\t Loss=3.04,\tTraining Accuracy=0.0%\n",
      "iter 1670:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 1680:\t Loss=2.46,\tTraining Accuracy=100.0%\n",
      "iter 1690:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 1700:\t Loss=3.16,\tTraining Accuracy=0.0%\n",
      "iter 1710:\t Loss=4.94,\tTraining Accuracy=0.0%\n",
      "iter 1720:\t Loss=2.76,\tTraining Accuracy=0.0%\n",
      "iter 1730:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 1740:\t Loss=3.10,\tTraining Accuracy=0.0%\n",
      "iter 1750:\t Loss=3.56,\tTraining Accuracy=0.0%\n",
      "iter 1760:\t Loss=2.21,\tTraining Accuracy=0.0%\n",
      "iter 1770:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter 1780:\t Loss=3.97,\tTraining Accuracy=0.0%\n",
      "iter 1790:\t Loss=2.83,\tTraining Accuracy=0.0%\n",
      "iter 1800:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 1810:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 1820:\t Loss=3.62,\tTraining Accuracy=0.0%\n",
      "iter 1830:\t Loss=2.64,\tTraining Accuracy=0.0%\n",
      "iter 1840:\t Loss=2.84,\tTraining Accuracy=0.0%\n",
      "iter 1850:\t Loss=3.81,\tTraining Accuracy=0.0%\n",
      "iter 1860:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 1870:\t Loss=1.94,\tTraining Accuracy=100.0%\n",
      "iter 1880:\t Loss=4.07,\tTraining Accuracy=0.0%\n",
      "iter 1890:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter 1900:\t Loss=2.74,\tTraining Accuracy=0.0%\n",
      "iter 1910:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 1920:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 1930:\t Loss=2.59,\tTraining Accuracy=0.0%\n",
      "iter 1940:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 1950:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 1960:\t Loss=2.53,\tTraining Accuracy=0.0%\n",
      "iter 1970:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 1980:\t Loss=2.99,\tTraining Accuracy=0.0%\n",
      "iter 1990:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 2000:\t Loss=2.81,\tTraining Accuracy=0.0%\n",
      "iter 2010:\t Loss=2.51,\tTraining Accuracy=0.0%\n",
      "iter 2020:\t Loss=2.87,\tTraining Accuracy=0.0%\n",
      "iter 2030:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 2040:\t Loss=4.17,\tTraining Accuracy=0.0%\n",
      "iter 2050:\t Loss=2.81,\tTraining Accuracy=0.0%\n",
      "iter 2060:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 2070:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 2080:\t Loss=2.20,\tTraining Accuracy=0.0%\n",
      "iter 2090:\t Loss=2.82,\tTraining Accuracy=0.0%\n",
      "iter 2100:\t Loss=2.73,\tTraining Accuracy=0.0%\n",
      "iter 2110:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 2120:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 2130:\t Loss=2.28,\tTraining Accuracy=0.0%\n",
      "iter 2140:\t Loss=4.38,\tTraining Accuracy=0.0%\n",
      "iter 2150:\t Loss=3.05,\tTraining Accuracy=0.0%\n",
      "iter 2160:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 2170:\t Loss=3.16,\tTraining Accuracy=0.0%\n",
      "iter 2180:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 2190:\t Loss=2.04,\tTraining Accuracy=100.0%\n",
      "iter 2200:\t Loss=2.50,\tTraining Accuracy=0.0%\n",
      "iter 2210:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 2220:\t Loss=2.61,\tTraining Accuracy=0.0%\n",
      "iter 2230:\t Loss=2.60,\tTraining Accuracy=0.0%\n",
      "iter 2240:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 2250:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter 2260:\t Loss=2.44,\tTraining Accuracy=0.0%\n",
      "iter 2270:\t Loss=4.52,\tTraining Accuracy=0.0%\n",
      "iter 2280:\t Loss=1.83,\tTraining Accuracy=100.0%\n",
      "iter 2290:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 2300:\t Loss=2.63,\tTraining Accuracy=0.0%\n",
      "iter 2310:\t Loss=2.86,\tTraining Accuracy=0.0%\n",
      "iter 2320:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 2330:\t Loss=3.04,\tTraining Accuracy=0.0%\n",
      "iter 2340:\t Loss=3.00,\tTraining Accuracy=0.0%\n",
      "iter 2350:\t Loss=2.93,\tTraining Accuracy=0.0%\n",
      "iter 2360:\t Loss=1.83,\tTraining Accuracy=100.0%\n",
      "iter 2370:\t Loss=2.80,\tTraining Accuracy=0.0%\n",
      "iter 2380:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 2390:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 2400:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 2410:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 2420:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 2430:\t Loss=2.68,\tTraining Accuracy=0.0%\n",
      "iter 2440:\t Loss=3.75,\tTraining Accuracy=0.0%\n",
      "iter 2450:\t Loss=2.65,\tTraining Accuracy=0.0%\n",
      "iter 2460:\t Loss=4.73,\tTraining Accuracy=0.0%\n",
      "iter 2470:\t Loss=2.43,\tTraining Accuracy=100.0%\n",
      "iter 2480:\t Loss=1.40,\tTraining Accuracy=100.0%\n",
      "iter 2490:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 2500:\t Loss=2.43,\tTraining Accuracy=0.0%\n",
      "iter 2510:\t Loss=2.25,\tTraining Accuracy=0.0%\n",
      "iter 2520:\t Loss=2.81,\tTraining Accuracy=0.0%\n",
      "iter 2530:\t Loss=3.77,\tTraining Accuracy=0.0%\n",
      "iter 2540:\t Loss=2.37,\tTraining Accuracy=100.0%\n",
      "iter 2550:\t Loss=1.37,\tTraining Accuracy=100.0%\n",
      "iter 2560:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter 2570:\t Loss=2.63,\tTraining Accuracy=0.0%\n",
      "iter 2580:\t Loss=2.60,\tTraining Accuracy=0.0%\n",
      "iter 2590:\t Loss=2.69,\tTraining Accuracy=0.0%\n",
      "iter 2600:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 2610:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 2620:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 2630:\t Loss=4.01,\tTraining Accuracy=0.0%\n",
      "iter 2640:\t Loss=2.94,\tTraining Accuracy=0.0%\n",
      "iter 2650:\t Loss=2.71,\tTraining Accuracy=0.0%\n",
      "iter 2660:\t Loss=2.56,\tTraining Accuracy=0.0%\n",
      "iter 2670:\t Loss=2.15,\tTraining Accuracy=0.0%\n",
      "iter 2680:\t Loss=2.47,\tTraining Accuracy=0.0%\n",
      "iter 2690:\t Loss=2.60,\tTraining Accuracy=0.0%\n",
      "iter 2700:\t Loss=2.95,\tTraining Accuracy=0.0%\n",
      "iter 2710:\t Loss=2.91,\tTraining Accuracy=0.0%\n",
      "iter 2720:\t Loss=3.16,\tTraining Accuracy=0.0%\n",
      "iter 2730:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 2740:\t Loss=3.21,\tTraining Accuracy=0.0%\n",
      "iter 2750:\t Loss=3.09,\tTraining Accuracy=100.0%\n",
      "iter 2760:\t Loss=2.47,\tTraining Accuracy=0.0%\n",
      "iter 2770:\t Loss=3.58,\tTraining Accuracy=0.0%\n",
      "iter 2780:\t Loss=3.92,\tTraining Accuracy=0.0%\n",
      "iter 2790:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter 2800:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 2810:\t Loss=2.54,\tTraining Accuracy=0.0%\n",
      "iter 2820:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 2830:\t Loss=3.87,\tTraining Accuracy=0.0%\n",
      "iter 2840:\t Loss=1.44,\tTraining Accuracy=100.0%\n",
      "iter 2850:\t Loss=2.64,\tTraining Accuracy=100.0%\n",
      "iter 2860:\t Loss=4.13,\tTraining Accuracy=0.0%\n",
      "iter 2870:\t Loss=2.91,\tTraining Accuracy=0.0%\n",
      "iter 2880:\t Loss=2.67,\tTraining Accuracy=0.0%\n",
      "iter 2890:\t Loss=2.32,\tTraining Accuracy=0.0%\n",
      "iter 2900:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter 2910:\t Loss=2.93,\tTraining Accuracy=0.0%\n",
      "iter 2920:\t Loss=4.09,\tTraining Accuracy=0.0%\n",
      "iter 2930:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 2940:\t Loss=2.25,\tTraining Accuracy=100.0%\n",
      "iter 2950:\t Loss=2.53,\tTraining Accuracy=0.0%\n",
      "iter 2960:\t Loss=3.01,\tTraining Accuracy=0.0%\n",
      "iter 2970:\t Loss=2.52,\tTraining Accuracy=0.0%\n",
      "iter 2980:\t Loss=3.16,\tTraining Accuracy=0.0%\n",
      "iter 2990:\t Loss=2.41,\tTraining Accuracy=100.0%\n",
      "iter 3000:\t Loss=3.10,\tTraining Accuracy=0.0%\n",
      "iter 3010:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 3020:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 3030:\t Loss=5.18,\tTraining Accuracy=0.0%\n",
      "iter 3040:\t Loss=1.97,\tTraining Accuracy=0.0%\n",
      "iter 3050:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter 3060:\t Loss=5.74,\tTraining Accuracy=0.0%\n",
      "iter 3070:\t Loss=2.35,\tTraining Accuracy=100.0%\n",
      "iter 3080:\t Loss=4.50,\tTraining Accuracy=0.0%\n",
      "iter 3090:\t Loss=3.04,\tTraining Accuracy=0.0%\n",
      "iter 3100:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 3110:\t Loss=2.36,\tTraining Accuracy=100.0%\n",
      "iter 3120:\t Loss=2.49,\tTraining Accuracy=0.0%\n",
      "iter 3130:\t Loss=2.40,\tTraining Accuracy=100.0%\n",
      "iter 3140:\t Loss=2.97,\tTraining Accuracy=0.0%\n",
      "iter 3150:\t Loss=2.69,\tTraining Accuracy=0.0%\n",
      "iter 3160:\t Loss=3.02,\tTraining Accuracy=0.0%\n",
      "iter 3170:\t Loss=1.98,\tTraining Accuracy=0.0%\n",
      "iter 3180:\t Loss=2.69,\tTraining Accuracy=100.0%\n",
      "iter 3190:\t Loss=3.54,\tTraining Accuracy=0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3200:\t Loss=2.79,\tTraining Accuracy=0.0%\n",
      "iter 3210:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 3220:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 3230:\t Loss=4.45,\tTraining Accuracy=0.0%\n",
      "iter 3240:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 3250:\t Loss=2.15,\tTraining Accuracy=0.0%\n",
      "iter 3260:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 3270:\t Loss=4.42,\tTraining Accuracy=0.0%\n",
      "iter 3280:\t Loss=2.92,\tTraining Accuracy=0.0%\n",
      "iter 3290:\t Loss=2.72,\tTraining Accuracy=0.0%\n",
      "iter 3300:\t Loss=2.92,\tTraining Accuracy=0.0%\n",
      "iter 3310:\t Loss=2.87,\tTraining Accuracy=0.0%\n",
      "iter 3320:\t Loss=2.46,\tTraining Accuracy=0.0%\n",
      "iter 3330:\t Loss=2.97,\tTraining Accuracy=0.0%\n",
      "iter 3340:\t Loss=3.79,\tTraining Accuracy=0.0%\n",
      "iter 3350:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 3360:\t Loss=2.79,\tTraining Accuracy=0.0%\n",
      "iter 3370:\t Loss=2.99,\tTraining Accuracy=0.0%\n",
      "iter 3380:\t Loss=3.21,\tTraining Accuracy=0.0%\n",
      "iter 3390:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter 3400:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 3410:\t Loss=2.42,\tTraining Accuracy=100.0%\n",
      "iter 3420:\t Loss=2.48,\tTraining Accuracy=0.0%\n",
      "iter 3430:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 3440:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 3450:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 3460:\t Loss=2.95,\tTraining Accuracy=0.0%\n",
      "iter 3470:\t Loss=3.13,\tTraining Accuracy=0.0%\n",
      "iter 3480:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 3490:\t Loss=1.74,\tTraining Accuracy=0.0%\n",
      "iter 3500:\t Loss=2.91,\tTraining Accuracy=0.0%\n",
      "iter 3510:\t Loss=2.00,\tTraining Accuracy=0.0%\n",
      "iter 3520:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 3530:\t Loss=2.97,\tTraining Accuracy=0.0%\n",
      "iter 3540:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 3550:\t Loss=2.88,\tTraining Accuracy=0.0%\n",
      "iter 3560:\t Loss=5.49,\tTraining Accuracy=0.0%\n",
      "iter 3570:\t Loss=3.76,\tTraining Accuracy=0.0%\n",
      "iter 3580:\t Loss=6.00,\tTraining Accuracy=0.0%\n",
      "iter 3590:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 3600:\t Loss=2.93,\tTraining Accuracy=0.0%\n",
      "iter 3610:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter 3620:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 3630:\t Loss=2.52,\tTraining Accuracy=0.0%\n",
      "iter 3640:\t Loss=2.89,\tTraining Accuracy=0.0%\n",
      "iter 3650:\t Loss=2.65,\tTraining Accuracy=0.0%\n",
      "iter 3660:\t Loss=2.48,\tTraining Accuracy=0.0%\n",
      "iter 3670:\t Loss=2.99,\tTraining Accuracy=0.0%\n",
      "iter 3680:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter 3690:\t Loss=3.74,\tTraining Accuracy=0.0%\n",
      "iter 3700:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 3710:\t Loss=2.76,\tTraining Accuracy=100.0%\n",
      "iter 3720:\t Loss=2.72,\tTraining Accuracy=0.0%\n",
      "iter 3730:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 3740:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 3750:\t Loss=2.59,\tTraining Accuracy=0.0%\n",
      "iter 3760:\t Loss=2.68,\tTraining Accuracy=0.0%\n",
      "iter 3770:\t Loss=2.23,\tTraining Accuracy=100.0%\n",
      "iter 3780:\t Loss=2.52,\tTraining Accuracy=0.0%\n",
      "iter 3790:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter 3800:\t Loss=3.81,\tTraining Accuracy=0.0%\n",
      "iter 3810:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter 3820:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 3830:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 3840:\t Loss=2.81,\tTraining Accuracy=0.0%\n",
      "iter 3850:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 3860:\t Loss=2.79,\tTraining Accuracy=0.0%\n",
      "iter 3870:\t Loss=2.75,\tTraining Accuracy=0.0%\n",
      "iter 3880:\t Loss=1.64,\tTraining Accuracy=100.0%\n",
      "iter 3890:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter 3900:\t Loss=2.67,\tTraining Accuracy=0.0%\n",
      "iter 3910:\t Loss=2.60,\tTraining Accuracy=0.0%\n",
      "iter 3920:\t Loss=3.67,\tTraining Accuracy=0.0%\n",
      "iter 3930:\t Loss=3.65,\tTraining Accuracy=0.0%\n",
      "iter 3940:\t Loss=2.89,\tTraining Accuracy=0.0%\n",
      "iter 3950:\t Loss=3.56,\tTraining Accuracy=0.0%\n",
      "iter 3960:\t Loss=3.63,\tTraining Accuracy=0.0%\n",
      "iter 3970:\t Loss=2.75,\tTraining Accuracy=100.0%\n",
      "iter 3980:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter 3990:\t Loss=2.57,\tTraining Accuracy=0.0%\n",
      "iter 4000:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter 4010:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 4020:\t Loss=2.97,\tTraining Accuracy=0.0%\n",
      "iter 4030:\t Loss=2.35,\tTraining Accuracy=100.0%\n",
      "iter 4040:\t Loss=2.26,\tTraining Accuracy=100.0%\n",
      "iter 4050:\t Loss=3.82,\tTraining Accuracy=0.0%\n",
      "iter 4060:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 4070:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 4080:\t Loss=5.85,\tTraining Accuracy=0.0%\n",
      "iter 4090:\t Loss=2.62,\tTraining Accuracy=0.0%\n",
      "iter 4100:\t Loss=4.91,\tTraining Accuracy=0.0%\n",
      "iter 4110:\t Loss=2.39,\tTraining Accuracy=0.0%\n",
      "iter 4120:\t Loss=3.06,\tTraining Accuracy=0.0%\n",
      "iter 4130:\t Loss=1.66,\tTraining Accuracy=100.0%\n",
      "iter 4140:\t Loss=2.40,\tTraining Accuracy=0.0%\n",
      "iter 4150:\t Loss=3.01,\tTraining Accuracy=0.0%\n",
      "iter 4160:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter 4170:\t Loss=4.17,\tTraining Accuracy=0.0%\n",
      "iter 4180:\t Loss=3.07,\tTraining Accuracy=0.0%\n",
      "iter 4190:\t Loss=3.04,\tTraining Accuracy=0.0%\n",
      "iter 4200:\t Loss=2.92,\tTraining Accuracy=0.0%\n",
      "iter 4210:\t Loss=2.71,\tTraining Accuracy=0.0%\n",
      "iter 4220:\t Loss=3.06,\tTraining Accuracy=0.0%\n",
      "iter 4230:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 4240:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "iter 4250:\t Loss=3.47,\tTraining Accuracy=0.0%\n",
      "iter 4260:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter 4270:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 4280:\t Loss=2.94,\tTraining Accuracy=0.0%\n",
      "iter 4290:\t Loss=4.02,\tTraining Accuracy=0.0%\n",
      "iter 4300:\t Loss=1.75,\tTraining Accuracy=100.0%\n",
      "iter 4310:\t Loss=3.16,\tTraining Accuracy=0.0%\n",
      "iter 4320:\t Loss=3.58,\tTraining Accuracy=0.0%\n",
      "iter 4330:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 4340:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 4350:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 4360:\t Loss=1.77,\tTraining Accuracy=100.0%\n",
      "iter 4370:\t Loss=2.88,\tTraining Accuracy=0.0%\n",
      "iter 4380:\t Loss=2.63,\tTraining Accuracy=0.0%\n",
      "iter 4390:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 4400:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 4410:\t Loss=4.66,\tTraining Accuracy=0.0%\n",
      "iter 4420:\t Loss=2.46,\tTraining Accuracy=0.0%\n",
      "iter 4430:\t Loss=2.30,\tTraining Accuracy=0.0%\n",
      "iter 4440:\t Loss=3.73,\tTraining Accuracy=0.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 4, validation loss: 3.23, validation accuracy: 11.9%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 5\n",
      "iter   0:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter  10:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter  20:\t Loss=2.13,\tTraining Accuracy=100.0%\n",
      "iter  30:\t Loss=2.86,\tTraining Accuracy=0.0%\n",
      "iter  40:\t Loss=3.59,\tTraining Accuracy=0.0%\n",
      "iter  50:\t Loss=3.56,\tTraining Accuracy=0.0%\n",
      "iter  60:\t Loss=2.54,\tTraining Accuracy=0.0%\n",
      "iter  70:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter  80:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter  90:\t Loss=3.33,\tTraining Accuracy=0.0%\n",
      "iter 100:\t Loss=2.15,\tTraining Accuracy=0.0%\n",
      "iter 110:\t Loss=3.02,\tTraining Accuracy=100.0%\n",
      "iter 120:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter 130:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 140:\t Loss=2.68,\tTraining Accuracy=0.0%\n",
      "iter 150:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 160:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "iter 170:\t Loss=2.99,\tTraining Accuracy=0.0%\n",
      "iter 180:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 190:\t Loss=2.98,\tTraining Accuracy=0.0%\n",
      "iter 200:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 210:\t Loss=2.65,\tTraining Accuracy=0.0%\n",
      "iter 220:\t Loss=2.27,\tTraining Accuracy=100.0%\n",
      "iter 230:\t Loss=2.97,\tTraining Accuracy=0.0%\n",
      "iter 240:\t Loss=2.49,\tTraining Accuracy=0.0%\n",
      "iter 250:\t Loss=1.46,\tTraining Accuracy=100.0%\n",
      "iter 260:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 270:\t Loss=2.99,\tTraining Accuracy=0.0%\n",
      "iter 280:\t Loss=3.03,\tTraining Accuracy=0.0%\n",
      "iter 290:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 300:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "iter 310:\t Loss=2.80,\tTraining Accuracy=0.0%\n",
      "iter 320:\t Loss=4.90,\tTraining Accuracy=0.0%\n",
      "iter 330:\t Loss=2.50,\tTraining Accuracy=0.0%\n",
      "iter 340:\t Loss=2.35,\tTraining Accuracy=0.0%\n",
      "iter 350:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 360:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 370:\t Loss=2.39,\tTraining Accuracy=100.0%\n",
      "iter 380:\t Loss=2.77,\tTraining Accuracy=0.0%\n",
      "iter 390:\t Loss=2.70,\tTraining Accuracy=0.0%\n",
      "iter 400:\t Loss=3.90,\tTraining Accuracy=0.0%\n",
      "iter 410:\t Loss=5.15,\tTraining Accuracy=0.0%\n",
      "iter 420:\t Loss=2.14,\tTraining Accuracy=0.0%\n",
      "iter 430:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 440:\t Loss=2.66,\tTraining Accuracy=0.0%\n",
      "iter 450:\t Loss=3.91,\tTraining Accuracy=0.0%\n",
      "iter 460:\t Loss=1.68,\tTraining Accuracy=100.0%\n",
      "iter 470:\t Loss=2.86,\tTraining Accuracy=0.0%\n",
      "iter 480:\t Loss=3.16,\tTraining Accuracy=0.0%\n",
      "iter 490:\t Loss=2.74,\tTraining Accuracy=0.0%\n",
      "iter 500:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter 510:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 520:\t Loss=3.10,\tTraining Accuracy=0.0%\n",
      "iter 530:\t Loss=3.00,\tTraining Accuracy=0.0%\n",
      "iter 540:\t Loss=2.88,\tTraining Accuracy=0.0%\n",
      "iter 550:\t Loss=2.50,\tTraining Accuracy=0.0%\n",
      "iter 560:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 570:\t Loss=1.78,\tTraining Accuracy=0.0%\n",
      "iter 580:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 590:\t Loss=3.99,\tTraining Accuracy=0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 600:\t Loss=2.48,\tTraining Accuracy=0.0%\n",
      "iter 610:\t Loss=3.25,\tTraining Accuracy=0.0%\n",
      "iter 620:\t Loss=2.64,\tTraining Accuracy=0.0%\n",
      "iter 630:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 640:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter 650:\t Loss=2.93,\tTraining Accuracy=0.0%\n",
      "iter 660:\t Loss=3.48,\tTraining Accuracy=0.0%\n",
      "iter 670:\t Loss=4.07,\tTraining Accuracy=0.0%\n",
      "iter 680:\t Loss=2.37,\tTraining Accuracy=0.0%\n",
      "iter 690:\t Loss=2.09,\tTraining Accuracy=100.0%\n",
      "iter 700:\t Loss=3.25,\tTraining Accuracy=0.0%\n",
      "iter 710:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter 720:\t Loss=3.04,\tTraining Accuracy=0.0%\n",
      "iter 730:\t Loss=3.25,\tTraining Accuracy=0.0%\n",
      "iter 740:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter 750:\t Loss=3.59,\tTraining Accuracy=0.0%\n",
      "iter 760:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 770:\t Loss=2.83,\tTraining Accuracy=100.0%\n",
      "iter 780:\t Loss=4.00,\tTraining Accuracy=0.0%\n",
      "iter 790:\t Loss=2.51,\tTraining Accuracy=0.0%\n",
      "iter 800:\t Loss=2.96,\tTraining Accuracy=0.0%\n",
      "iter 810:\t Loss=2.76,\tTraining Accuracy=0.0%\n",
      "iter 820:\t Loss=2.35,\tTraining Accuracy=100.0%\n",
      "iter 830:\t Loss=4.10,\tTraining Accuracy=0.0%\n",
      "iter 840:\t Loss=2.48,\tTraining Accuracy=0.0%\n",
      "iter 850:\t Loss=2.58,\tTraining Accuracy=0.0%\n",
      "iter 860:\t Loss=2.38,\tTraining Accuracy=0.0%\n",
      "iter 870:\t Loss=3.10,\tTraining Accuracy=0.0%\n",
      "iter 880:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 890:\t Loss=2.53,\tTraining Accuracy=0.0%\n",
      "iter 900:\t Loss=2.67,\tTraining Accuracy=0.0%\n",
      "iter 910:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 920:\t Loss=2.48,\tTraining Accuracy=0.0%\n",
      "iter 930:\t Loss=2.94,\tTraining Accuracy=0.0%\n",
      "iter 940:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 950:\t Loss=5.14,\tTraining Accuracy=0.0%\n",
      "iter 960:\t Loss=3.56,\tTraining Accuracy=0.0%\n",
      "iter 970:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 980:\t Loss=3.18,\tTraining Accuracy=100.0%\n",
      "iter 990:\t Loss=1.47,\tTraining Accuracy=100.0%\n",
      "iter 1000:\t Loss=1.39,\tTraining Accuracy=100.0%\n",
      "iter 1010:\t Loss=2.50,\tTraining Accuracy=0.0%\n",
      "iter 1020:\t Loss=3.20,\tTraining Accuracy=0.0%\n",
      "iter 1030:\t Loss=3.25,\tTraining Accuracy=0.0%\n",
      "iter 1040:\t Loss=3.85,\tTraining Accuracy=0.0%\n",
      "iter 1050:\t Loss=3.99,\tTraining Accuracy=0.0%\n",
      "iter 1060:\t Loss=2.67,\tTraining Accuracy=100.0%\n",
      "iter 1070:\t Loss=3.79,\tTraining Accuracy=0.0%\n",
      "iter 1080:\t Loss=1.41,\tTraining Accuracy=100.0%\n",
      "iter 1090:\t Loss=3.75,\tTraining Accuracy=0.0%\n",
      "iter 1100:\t Loss=2.92,\tTraining Accuracy=0.0%\n",
      "iter 1110:\t Loss=1.96,\tTraining Accuracy=100.0%\n",
      "iter 1120:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 1130:\t Loss=2.25,\tTraining Accuracy=0.0%\n",
      "iter 1140:\t Loss=3.68,\tTraining Accuracy=0.0%\n",
      "iter 1150:\t Loss=2.47,\tTraining Accuracy=0.0%\n",
      "iter 1160:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 1170:\t Loss=2.35,\tTraining Accuracy=0.0%\n",
      "iter 1180:\t Loss=3.02,\tTraining Accuracy=0.0%\n",
      "iter 1190:\t Loss=2.29,\tTraining Accuracy=100.0%\n",
      "iter 1200:\t Loss=2.99,\tTraining Accuracy=0.0%\n",
      "iter 1210:\t Loss=2.98,\tTraining Accuracy=0.0%\n",
      "iter 1220:\t Loss=1.45,\tTraining Accuracy=100.0%\n",
      "iter 1230:\t Loss=2.14,\tTraining Accuracy=100.0%\n",
      "iter 1240:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 1250:\t Loss=1.87,\tTraining Accuracy=100.0%\n",
      "iter 1260:\t Loss=4.20,\tTraining Accuracy=0.0%\n",
      "iter 1270:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 1280:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 1290:\t Loss=2.79,\tTraining Accuracy=0.0%\n",
      "iter 1300:\t Loss=1.70,\tTraining Accuracy=100.0%\n",
      "iter 1310:\t Loss=5.54,\tTraining Accuracy=0.0%\n",
      "iter 1320:\t Loss=2.31,\tTraining Accuracy=0.0%\n",
      "iter 1330:\t Loss=4.08,\tTraining Accuracy=0.0%\n",
      "iter 1340:\t Loss=2.61,\tTraining Accuracy=0.0%\n",
      "iter 1350:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 1360:\t Loss=2.07,\tTraining Accuracy=100.0%\n",
      "iter 1370:\t Loss=2.35,\tTraining Accuracy=0.0%\n",
      "iter 1380:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 1390:\t Loss=2.70,\tTraining Accuracy=0.0%\n",
      "iter 1400:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 1410:\t Loss=2.68,\tTraining Accuracy=0.0%\n",
      "iter 1420:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 1430:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter 1440:\t Loss=2.63,\tTraining Accuracy=0.0%\n",
      "iter 1450:\t Loss=2.84,\tTraining Accuracy=0.0%\n",
      "iter 1460:\t Loss=2.79,\tTraining Accuracy=0.0%\n",
      "iter 1470:\t Loss=1.86,\tTraining Accuracy=0.0%\n",
      "iter 1480:\t Loss=8.45,\tTraining Accuracy=0.0%\n",
      "iter 1490:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 1500:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 1510:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 1520:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 1530:\t Loss=3.10,\tTraining Accuracy=0.0%\n",
      "iter 1540:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 1550:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 1560:\t Loss=2.74,\tTraining Accuracy=0.0%\n",
      "iter 1570:\t Loss=4.46,\tTraining Accuracy=0.0%\n",
      "iter 1580:\t Loss=2.85,\tTraining Accuracy=0.0%\n",
      "iter 1590:\t Loss=3.00,\tTraining Accuracy=0.0%\n",
      "iter 1600:\t Loss=3.97,\tTraining Accuracy=0.0%\n",
      "iter 1610:\t Loss=2.71,\tTraining Accuracy=0.0%\n",
      "iter 1620:\t Loss=2.66,\tTraining Accuracy=0.0%\n",
      "iter 1630:\t Loss=2.64,\tTraining Accuracy=0.0%\n",
      "iter 1640:\t Loss=3.62,\tTraining Accuracy=0.0%\n",
      "iter 1650:\t Loss=2.65,\tTraining Accuracy=0.0%\n",
      "iter 1660:\t Loss=2.39,\tTraining Accuracy=100.0%\n",
      "iter 1670:\t Loss=2.40,\tTraining Accuracy=0.0%\n",
      "iter 1680:\t Loss=2.74,\tTraining Accuracy=0.0%\n",
      "iter 1690:\t Loss=2.34,\tTraining Accuracy=0.0%\n",
      "iter 1700:\t Loss=2.80,\tTraining Accuracy=0.0%\n",
      "iter 1710:\t Loss=2.45,\tTraining Accuracy=0.0%\n",
      "iter 1720:\t Loss=2.21,\tTraining Accuracy=0.0%\n",
      "iter 1730:\t Loss=2.63,\tTraining Accuracy=0.0%\n",
      "iter 1740:\t Loss=3.21,\tTraining Accuracy=0.0%\n",
      "iter 1750:\t Loss=3.62,\tTraining Accuracy=0.0%\n",
      "iter 1760:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 1770:\t Loss=3.57,\tTraining Accuracy=0.0%\n",
      "iter 1780:\t Loss=2.85,\tTraining Accuracy=0.0%\n",
      "iter 1790:\t Loss=2.63,\tTraining Accuracy=0.0%\n",
      "iter 1800:\t Loss=2.46,\tTraining Accuracy=100.0%\n",
      "iter 1810:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 1820:\t Loss=2.52,\tTraining Accuracy=100.0%\n",
      "iter 1830:\t Loss=2.77,\tTraining Accuracy=0.0%\n",
      "iter 1840:\t Loss=3.37,\tTraining Accuracy=0.0%\n",
      "iter 1850:\t Loss=3.12,\tTraining Accuracy=0.0%\n",
      "iter 1860:\t Loss=3.03,\tTraining Accuracy=0.0%\n",
      "iter 1870:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 1880:\t Loss=3.62,\tTraining Accuracy=0.0%\n",
      "iter 1890:\t Loss=2.22,\tTraining Accuracy=100.0%\n",
      "iter 1900:\t Loss=3.32,\tTraining Accuracy=0.0%\n",
      "iter 1910:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter 1920:\t Loss=2.82,\tTraining Accuracy=0.0%\n",
      "iter 1930:\t Loss=3.04,\tTraining Accuracy=0.0%\n",
      "iter 1940:\t Loss=2.14,\tTraining Accuracy=100.0%\n",
      "iter 1950:\t Loss=3.07,\tTraining Accuracy=0.0%\n",
      "iter 1960:\t Loss=1.87,\tTraining Accuracy=0.0%\n",
      "iter 1970:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 1980:\t Loss=2.83,\tTraining Accuracy=0.0%\n",
      "iter 1990:\t Loss=1.89,\tTraining Accuracy=0.0%\n",
      "iter 2000:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 2010:\t Loss=3.00,\tTraining Accuracy=0.0%\n",
      "iter 2020:\t Loss=2.53,\tTraining Accuracy=0.0%\n",
      "iter 2030:\t Loss=2.89,\tTraining Accuracy=0.0%\n",
      "iter 2040:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 2050:\t Loss=2.21,\tTraining Accuracy=0.0%\n",
      "iter 2060:\t Loss=3.21,\tTraining Accuracy=0.0%\n",
      "iter 2070:\t Loss=3.01,\tTraining Accuracy=0.0%\n",
      "iter 2080:\t Loss=2.47,\tTraining Accuracy=0.0%\n",
      "iter 2090:\t Loss=4.09,\tTraining Accuracy=0.0%\n",
      "iter 2100:\t Loss=2.47,\tTraining Accuracy=100.0%\n",
      "iter 2110:\t Loss=2.80,\tTraining Accuracy=0.0%\n",
      "iter 2120:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 2130:\t Loss=4.00,\tTraining Accuracy=0.0%\n",
      "iter 2140:\t Loss=2.97,\tTraining Accuracy=0.0%\n",
      "iter 2150:\t Loss=1.23,\tTraining Accuracy=100.0%\n",
      "iter 2160:\t Loss=2.89,\tTraining Accuracy=0.0%\n",
      "iter 2170:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter 2180:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 2190:\t Loss=2.89,\tTraining Accuracy=0.0%\n",
      "iter 2200:\t Loss=2.43,\tTraining Accuracy=0.0%\n",
      "iter 2210:\t Loss=2.47,\tTraining Accuracy=0.0%\n",
      "iter 2220:\t Loss=1.93,\tTraining Accuracy=0.0%\n",
      "iter 2230:\t Loss=3.95,\tTraining Accuracy=0.0%\n",
      "iter 2240:\t Loss=2.93,\tTraining Accuracy=0.0%\n",
      "iter 2250:\t Loss=3.10,\tTraining Accuracy=0.0%\n",
      "iter 2260:\t Loss=1.97,\tTraining Accuracy=100.0%\n",
      "iter 2270:\t Loss=2.64,\tTraining Accuracy=100.0%\n",
      "iter 2280:\t Loss=2.30,\tTraining Accuracy=100.0%\n",
      "iter 2290:\t Loss=1.05,\tTraining Accuracy=100.0%\n",
      "iter 2300:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 2310:\t Loss=2.76,\tTraining Accuracy=0.0%\n",
      "iter 2320:\t Loss=2.46,\tTraining Accuracy=0.0%\n",
      "iter 2330:\t Loss=2.22,\tTraining Accuracy=100.0%\n",
      "iter 2340:\t Loss=2.83,\tTraining Accuracy=0.0%\n",
      "iter 2350:\t Loss=1.98,\tTraining Accuracy=0.0%\n",
      "iter 2360:\t Loss=2.18,\tTraining Accuracy=0.0%\n",
      "iter 2370:\t Loss=2.45,\tTraining Accuracy=100.0%\n",
      "iter 2380:\t Loss=1.91,\tTraining Accuracy=100.0%\n",
      "iter 2390:\t Loss=1.88,\tTraining Accuracy=100.0%\n",
      "iter 2400:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 2410:\t Loss=3.03,\tTraining Accuracy=0.0%\n",
      "iter 2420:\t Loss=2.46,\tTraining Accuracy=0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2430:\t Loss=2.30,\tTraining Accuracy=0.0%\n",
      "iter 2440:\t Loss=3.94,\tTraining Accuracy=0.0%\n",
      "iter 2450:\t Loss=2.20,\tTraining Accuracy=100.0%\n",
      "iter 2460:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter 2470:\t Loss=2.20,\tTraining Accuracy=100.0%\n",
      "iter 2480:\t Loss=3.12,\tTraining Accuracy=0.0%\n",
      "iter 2490:\t Loss=3.65,\tTraining Accuracy=0.0%\n",
      "iter 2500:\t Loss=2.54,\tTraining Accuracy=0.0%\n",
      "iter 2510:\t Loss=3.02,\tTraining Accuracy=0.0%\n",
      "iter 2520:\t Loss=2.26,\tTraining Accuracy=0.0%\n",
      "iter 2530:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 2540:\t Loss=2.53,\tTraining Accuracy=0.0%\n",
      "iter 2550:\t Loss=3.77,\tTraining Accuracy=0.0%\n",
      "iter 2560:\t Loss=3.84,\tTraining Accuracy=0.0%\n",
      "iter 2570:\t Loss=2.69,\tTraining Accuracy=0.0%\n",
      "iter 2580:\t Loss=2.90,\tTraining Accuracy=0.0%\n",
      "iter 2590:\t Loss=4.57,\tTraining Accuracy=0.0%\n",
      "iter 2600:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 2610:\t Loss=3.66,\tTraining Accuracy=0.0%\n",
      "iter 2620:\t Loss=3.29,\tTraining Accuracy=0.0%\n",
      "iter 2630:\t Loss=2.73,\tTraining Accuracy=0.0%\n",
      "iter 2640:\t Loss=2.36,\tTraining Accuracy=0.0%\n",
      "iter 2650:\t Loss=3.58,\tTraining Accuracy=0.0%\n",
      "iter 2660:\t Loss=3.50,\tTraining Accuracy=0.0%\n",
      "iter 2670:\t Loss=2.49,\tTraining Accuracy=0.0%\n",
      "iter 2680:\t Loss=3.20,\tTraining Accuracy=0.0%\n",
      "iter 2690:\t Loss=2.85,\tTraining Accuracy=0.0%\n",
      "iter 2700:\t Loss=2.99,\tTraining Accuracy=0.0%\n",
      "iter 2710:\t Loss=6.57,\tTraining Accuracy=0.0%\n",
      "iter 2720:\t Loss=3.10,\tTraining Accuracy=0.0%\n",
      "iter 2730:\t Loss=3.05,\tTraining Accuracy=0.0%\n",
      "iter 2740:\t Loss=2.47,\tTraining Accuracy=0.0%\n",
      "iter 2750:\t Loss=5.25,\tTraining Accuracy=0.0%\n",
      "iter 2760:\t Loss=4.14,\tTraining Accuracy=0.0%\n",
      "iter 2770:\t Loss=2.92,\tTraining Accuracy=0.0%\n",
      "iter 2780:\t Loss=3.03,\tTraining Accuracy=0.0%\n",
      "iter 2790:\t Loss=2.57,\tTraining Accuracy=0.0%\n",
      "iter 2800:\t Loss=4.98,\tTraining Accuracy=0.0%\n",
      "iter 2810:\t Loss=4.23,\tTraining Accuracy=0.0%\n",
      "iter 2820:\t Loss=1.56,\tTraining Accuracy=100.0%\n",
      "iter 2830:\t Loss=2.37,\tTraining Accuracy=0.0%\n",
      "iter 2840:\t Loss=2.97,\tTraining Accuracy=0.0%\n",
      "iter 2850:\t Loss=2.59,\tTraining Accuracy=0.0%\n",
      "iter 2860:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 2870:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter 2880:\t Loss=3.82,\tTraining Accuracy=0.0%\n",
      "iter 2890:\t Loss=2.42,\tTraining Accuracy=0.0%\n",
      "iter 2900:\t Loss=2.41,\tTraining Accuracy=0.0%\n",
      "iter 2910:\t Loss=2.50,\tTraining Accuracy=0.0%\n",
      "iter 2920:\t Loss=2.96,\tTraining Accuracy=0.0%\n",
      "iter 2930:\t Loss=2.42,\tTraining Accuracy=100.0%\n",
      "iter 2940:\t Loss=3.84,\tTraining Accuracy=0.0%\n",
      "iter 2950:\t Loss=2.73,\tTraining Accuracy=0.0%\n",
      "iter 2960:\t Loss=2.40,\tTraining Accuracy=0.0%\n",
      "iter 2970:\t Loss=2.07,\tTraining Accuracy=100.0%\n",
      "iter 2980:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 2990:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 3000:\t Loss=4.50,\tTraining Accuracy=0.0%\n",
      "iter 3010:\t Loss=2.32,\tTraining Accuracy=0.0%\n",
      "iter 3020:\t Loss=2.47,\tTraining Accuracy=0.0%\n",
      "iter 3030:\t Loss=2.77,\tTraining Accuracy=0.0%\n",
      "iter 3040:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "iter 3050:\t Loss=5.66,\tTraining Accuracy=0.0%\n",
      "iter 3060:\t Loss=2.81,\tTraining Accuracy=0.0%\n",
      "iter 3070:\t Loss=1.32,\tTraining Accuracy=100.0%\n",
      "iter 3080:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter 3090:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "iter 3100:\t Loss=3.03,\tTraining Accuracy=0.0%\n",
      "iter 3110:\t Loss=2.89,\tTraining Accuracy=0.0%\n",
      "iter 3120:\t Loss=3.88,\tTraining Accuracy=0.0%\n",
      "iter 3130:\t Loss=2.46,\tTraining Accuracy=0.0%\n",
      "iter 3140:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter 3150:\t Loss=2.15,\tTraining Accuracy=0.0%\n",
      "iter 3160:\t Loss=3.57,\tTraining Accuracy=0.0%\n",
      "iter 3170:\t Loss=2.90,\tTraining Accuracy=0.0%\n",
      "iter 3180:\t Loss=1.89,\tTraining Accuracy=100.0%\n",
      "iter 3190:\t Loss=2.75,\tTraining Accuracy=0.0%\n",
      "iter 3200:\t Loss=2.98,\tTraining Accuracy=0.0%\n",
      "iter 3210:\t Loss=3.71,\tTraining Accuracy=0.0%\n",
      "iter 3220:\t Loss=2.19,\tTraining Accuracy=100.0%\n",
      "iter 3230:\t Loss=3.15,\tTraining Accuracy=0.0%\n",
      "iter 3240:\t Loss=2.76,\tTraining Accuracy=0.0%\n",
      "iter 3250:\t Loss=4.47,\tTraining Accuracy=0.0%\n",
      "iter 3260:\t Loss=2.15,\tTraining Accuracy=0.0%\n",
      "iter 3270:\t Loss=3.02,\tTraining Accuracy=0.0%\n",
      "iter 3280:\t Loss=2.60,\tTraining Accuracy=0.0%\n",
      "iter 3290:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter 3300:\t Loss=2.58,\tTraining Accuracy=0.0%\n",
      "iter 3310:\t Loss=3.02,\tTraining Accuracy=0.0%\n",
      "iter 3320:\t Loss=3.02,\tTraining Accuracy=0.0%\n",
      "iter 3330:\t Loss=3.64,\tTraining Accuracy=0.0%\n",
      "iter 3340:\t Loss=2.46,\tTraining Accuracy=0.0%\n",
      "iter 3350:\t Loss=3.69,\tTraining Accuracy=0.0%\n",
      "iter 3360:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 3370:\t Loss=1.85,\tTraining Accuracy=100.0%\n",
      "iter 3380:\t Loss=2.10,\tTraining Accuracy=0.0%\n",
      "iter 3390:\t Loss=2.34,\tTraining Accuracy=0.0%\n",
      "iter 3400:\t Loss=3.68,\tTraining Accuracy=0.0%\n",
      "iter 3410:\t Loss=2.69,\tTraining Accuracy=0.0%\n",
      "iter 3420:\t Loss=4.11,\tTraining Accuracy=0.0%\n",
      "iter 3430:\t Loss=2.63,\tTraining Accuracy=100.0%\n",
      "iter 3440:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 3450:\t Loss=2.78,\tTraining Accuracy=0.0%\n",
      "iter 3460:\t Loss=2.70,\tTraining Accuracy=0.0%\n",
      "iter 3470:\t Loss=4.20,\tTraining Accuracy=0.0%\n",
      "iter 3480:\t Loss=2.48,\tTraining Accuracy=0.0%\n",
      "iter 3490:\t Loss=3.20,\tTraining Accuracy=0.0%\n",
      "iter 3500:\t Loss=4.05,\tTraining Accuracy=0.0%\n",
      "iter 3510:\t Loss=2.57,\tTraining Accuracy=0.0%\n",
      "iter 3520:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 3530:\t Loss=2.06,\tTraining Accuracy=100.0%\n",
      "iter 3540:\t Loss=1.92,\tTraining Accuracy=100.0%\n",
      "iter 3550:\t Loss=2.79,\tTraining Accuracy=0.0%\n",
      "iter 3560:\t Loss=2.39,\tTraining Accuracy=0.0%\n",
      "iter 3570:\t Loss=4.50,\tTraining Accuracy=0.0%\n",
      "iter 3580:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter 3590:\t Loss=2.17,\tTraining Accuracy=100.0%\n",
      "iter 3600:\t Loss=3.03,\tTraining Accuracy=0.0%\n",
      "iter 3610:\t Loss=2.33,\tTraining Accuracy=0.0%\n",
      "iter 3620:\t Loss=3.57,\tTraining Accuracy=0.0%\n",
      "iter 3630:\t Loss=4.51,\tTraining Accuracy=0.0%\n",
      "iter 3640:\t Loss=2.57,\tTraining Accuracy=100.0%\n",
      "iter 3650:\t Loss=2.45,\tTraining Accuracy=0.0%\n",
      "iter 3660:\t Loss=1.51,\tTraining Accuracy=100.0%\n",
      "iter 3670:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 3680:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 3690:\t Loss=7.15,\tTraining Accuracy=0.0%\n",
      "iter 3700:\t Loss=2.91,\tTraining Accuracy=0.0%\n",
      "iter 3710:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 3720:\t Loss=2.59,\tTraining Accuracy=0.0%\n",
      "iter 3730:\t Loss=1.16,\tTraining Accuracy=100.0%\n",
      "iter 3740:\t Loss=1.98,\tTraining Accuracy=0.0%\n",
      "iter 3750:\t Loss=2.89,\tTraining Accuracy=0.0%\n",
      "iter 3760:\t Loss=3.90,\tTraining Accuracy=0.0%\n",
      "iter 3770:\t Loss=2.16,\tTraining Accuracy=0.0%\n",
      "iter 3780:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter 3790:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter 3800:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter 3810:\t Loss=2.54,\tTraining Accuracy=0.0%\n",
      "iter 3820:\t Loss=2.78,\tTraining Accuracy=0.0%\n",
      "iter 3830:\t Loss=3.34,\tTraining Accuracy=0.0%\n",
      "iter 3840:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 3850:\t Loss=2.82,\tTraining Accuracy=0.0%\n",
      "iter 3860:\t Loss=2.00,\tTraining Accuracy=100.0%\n",
      "iter 3870:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 3880:\t Loss=2.59,\tTraining Accuracy=0.0%\n",
      "iter 3890:\t Loss=2.84,\tTraining Accuracy=0.0%\n",
      "iter 3900:\t Loss=5.96,\tTraining Accuracy=0.0%\n",
      "iter 3910:\t Loss=1.37,\tTraining Accuracy=100.0%\n",
      "iter 3920:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 3930:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 3940:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter 3950:\t Loss=3.26,\tTraining Accuracy=0.0%\n",
      "iter 3960:\t Loss=2.91,\tTraining Accuracy=0.0%\n",
      "iter 3970:\t Loss=3.04,\tTraining Accuracy=0.0%\n",
      "iter 3980:\t Loss=2.43,\tTraining Accuracy=0.0%\n",
      "iter 3990:\t Loss=3.19,\tTraining Accuracy=0.0%\n",
      "iter 4000:\t Loss=2.13,\tTraining Accuracy=0.0%\n",
      "iter 4010:\t Loss=2.60,\tTraining Accuracy=0.0%\n",
      "iter 4020:\t Loss=4.13,\tTraining Accuracy=0.0%\n",
      "iter 4030:\t Loss=2.66,\tTraining Accuracy=0.0%\n",
      "iter 4040:\t Loss=2.67,\tTraining Accuracy=0.0%\n",
      "iter 4050:\t Loss=3.46,\tTraining Accuracy=0.0%\n",
      "iter 4060:\t Loss=2.68,\tTraining Accuracy=0.0%\n",
      "iter 4070:\t Loss=3.23,\tTraining Accuracy=0.0%\n",
      "iter 4080:\t Loss=2.83,\tTraining Accuracy=0.0%\n",
      "iter 4090:\t Loss=2.22,\tTraining Accuracy=100.0%\n",
      "iter 4100:\t Loss=4.54,\tTraining Accuracy=0.0%\n",
      "iter 4110:\t Loss=2.22,\tTraining Accuracy=100.0%\n",
      "iter 4120:\t Loss=2.80,\tTraining Accuracy=0.0%\n",
      "iter 4130:\t Loss=3.14,\tTraining Accuracy=0.0%\n",
      "iter 4140:\t Loss=2.26,\tTraining Accuracy=0.0%\n",
      "iter 4150:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter 4160:\t Loss=3.11,\tTraining Accuracy=0.0%\n",
      "iter 4170:\t Loss=2.62,\tTraining Accuracy=0.0%\n",
      "iter 4180:\t Loss=2.54,\tTraining Accuracy=100.0%\n",
      "iter 4190:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 4200:\t Loss=2.86,\tTraining Accuracy=0.0%\n",
      "iter 4210:\t Loss=2.59,\tTraining Accuracy=0.0%\n",
      "iter 4220:\t Loss=3.27,\tTraining Accuracy=0.0%\n",
      "iter 4230:\t Loss=3.91,\tTraining Accuracy=0.0%\n",
      "iter 4240:\t Loss=2.46,\tTraining Accuracy=100.0%\n",
      "iter 4250:\t Loss=3.03,\tTraining Accuracy=0.0%\n",
      "iter 4260:\t Loss=1.66,\tTraining Accuracy=100.0%\n",
      "iter 4270:\t Loss=3.42,\tTraining Accuracy=0.0%\n",
      "iter 4280:\t Loss=3.24,\tTraining Accuracy=0.0%\n",
      "iter 4290:\t Loss=3.33,\tTraining Accuracy=0.0%\n",
      "iter 4300:\t Loss=2.16,\tTraining Accuracy=100.0%\n",
      "iter 4310:\t Loss=3.25,\tTraining Accuracy=0.0%\n",
      "iter 4320:\t Loss=1.79,\tTraining Accuracy=100.0%\n",
      "iter 4330:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter 4340:\t Loss=3.17,\tTraining Accuracy=0.0%\n",
      "iter 4350:\t Loss=3.02,\tTraining Accuracy=0.0%\n",
      "iter 4360:\t Loss=3.18,\tTraining Accuracy=0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4370:\t Loss=2.92,\tTraining Accuracy=0.0%\n",
      "iter 4380:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter 4390:\t Loss=3.20,\tTraining Accuracy=0.0%\n",
      "iter 4400:\t Loss=3.18,\tTraining Accuracy=0.0%\n",
      "iter 4410:\t Loss=2.35,\tTraining Accuracy=0.0%\n",
      "iter 4420:\t Loss=3.09,\tTraining Accuracy=0.0%\n",
      "iter 4430:\t Loss=2.24,\tTraining Accuracy=0.0%\n",
      "iter 4440:\t Loss=3.30,\tTraining Accuracy=0.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 5, validation loss: 3.17, validation accuracy: 13.1%\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph (session)\n",
    "sess = tf.InteractiveSession() # using InteractiveSession instead of Session to test network in separate cell\n",
    "sess.run(init)\n",
    "# Number of training iterations in each epoch\n",
    "num_tr_iter = int(trainNumber / batch_size)\n",
    "for epoch in range(epochs):\n",
    "    print('Training epoch: {}'.format(epoch+1))\n",
    "    for iteration in range(num_tr_iter):\n",
    "        #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x, batch_y = next_batch(batch_size,train_flat_images,trainLabels)\n",
    "        # Run optimization op (backprop)\n",
    "        feed_dict_batch = {x: batch_x, y: batch_y}\n",
    "        sess.run(optimizer, feed_dict=feed_dict_batch)\n",
    "\n",
    "        if iteration % display_freq == 0:\n",
    "            # Calculate and display the batch loss and accuracy\n",
    "            loss_batch, acc_batch = sess.run([loss, accuracy],\n",
    "                                             feed_dict=feed_dict_batch)\n",
    "            print(\"iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}\".\n",
    "                  format(iteration, loss_batch, acc_batch))\n",
    "\n",
    "    # Run validation after every epoch\n",
    "    feed_dict_valid = {x: val_flat_images, y: validationLabels}\n",
    "    loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
    "          format(epoch + 1, loss_valid, acc_valid))\n",
    "    print('---------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None, title=None):\n",
    "    \"\"\"\n",
    "    Create figure with 3x3 sub-plots.\n",
    "    :param images: array of images to be plotted, (9, img_h*img_w)\n",
    "    :param cls_true: corresponding true labels (9,)\n",
    "    :param cls_pred: corresponding true labels (9,)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    img_h = 16\n",
    "    img_w = 8 \n",
    "    #np.sqrt(images.shape[-1]).astype(int)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape((img_h, img_w)), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            ax_title = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            ax_title = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_title(ax_title)\n",
    "\n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    if title:\n",
    "        plt.suptitle(title, size=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example_errors(images, cls_true, cls_pred, title=None):\n",
    "    \"\"\"\n",
    "    Function for plotting examples of images that have been mis-classified\n",
    "    :param images: array of all images, (#imgs, img_h*img_w)\n",
    "    :param cls_true: corresponding true labels, (#imgs,)\n",
    "    :param cls_pred: corresponding predicted labels, (#imgs,)\n",
    "    \"\"\"\n",
    "    # Negate the boolean array.\n",
    "    incorrect = np.logical_not(np.equal(cls_pred, cls_true))\n",
    "\n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    incorrect_images = images[incorrect]\n",
    "\n",
    "    # Get the true and predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "    cls_true = cls_true[incorrect]\n",
    "\n",
    "    # Plot the first 9 images.\n",
    "    plot_images(images=incorrect_images[0:31],\n",
    "                cls_true=cls_true[0:31],\n",
    "                cls_pred=cls_pred[0:31],\n",
    "                title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flat_images[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testLabels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Test loss: 3.17, test accuracy: 13.1%\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAI7CAYAAADS7IOgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VdW99/HvLwmBMCSACiijE3UAFccCShFHHNraWh9rHdvqva3Py6GtA3UesL1PvY96q/ZqvUpFrThULNqKT62gKFaNaB2gisgkcxgDYUiynj/2Tj2NOWudkMPJInzer9d5JTm/fX5772TlfPcZ1j7mnBMAAGh9Ra29AQAAIEEoAwAQCUIZAIBIEMoAAESCUAYAIBKEMgAAkSCUAUTNzEaamTOzG1t7W4BtjVDGNmFm+5jZr83sAzNbY2abzWyRmT1vZj8ws/atvY3bkplNMbNmnwTAzM5PA8h3mbsNNhlABEpaewPQ9pjZ9ZJuUHLQN13S7yRVS+opaaSkByT9SNKhrbSJ24P3JE3MUltdyA0BUDiEMvLKzH4u6SZJCyR9xzn3tyaWOUXSTwu9bduZd51zN7b2RgAoLJ6+Rt6Y2QBJN0raIumkpgJZkpxzz0k6sYnbn2Fmr6RPd9eY2ftmNqapp7rNbG56KTez/5t+v6XhdUczuzF9qnekmZ1lZn8zs+rGT/2a2RFm9pSZLUmfYl9gZveZ2W5Z9rG7mY1Nn5bfkG7re2b2SzPrZGYD0qetv5Yun/m085Scf5k5MrPL095PN1E71szq0t9jWcb13zSzR8zsYzNbn14qzewSM/vSfYKZjUvXsbuZ/W8z+8jMNqa/85+bmaXLfcfM3kz7LTOzuzPXm9HPpU/v72Zm49Nla9JtOKuZ+9/dzH5hZjPTHmvM7CUzO76JZUvTfXzHzFalf7+5ZvasmR3bnPUC2wqPlJFPF0hqJ+lx59wHvgWdc5syfzaz2ySNkbRC0mNKnu4eLek2SSeY2fHOuc2N2pRK+quk7pJelLRW0meNlvmppOMkTZL0sqSKjHV+X9L9kjZJ+qOSR/d7S/qhpFPN7KvOufkZy++e9ugvqVLSb5Qc2A6UdLmk/1by1PJNks5Pl7spY1vm+n4nW8M5d4eZjZL0LTP7sXPu3nRbe0l6RNJGSWc452oybvZLSfWS/ibpcyW/k1GS7pJ0mKRzsqzudiUvP0xS8vv+uqSxkkrNbGXad6KkV5X8zi+WVKzkpYrGukl6Xcnv6yFJXSWdIelRM+vtnPtVaN/NrL+kKZIGpOt8QVInSadIesHM/s0599uMm4yT9F1JH0h6WFKNpN0kHankIPEvoXUC25xzjguXvFwkvSTJSfphM283NL3dfEm9Mq4vURIATtLPG91mbnr9XyR1aqLnjWl9vaQhTdQHStosabak3o1qx0iqk/RMo+tfT3uOaaLfzpI6ZPw8Jfn3avbv8Px0He+m+9DU5cRGt9lJyQFFjaQDlRwo/CXtc0ET69izieuKlLz27yQd0ag2Lr1+bubvSkmQrkh/x8sl7ZtRay/pIyUHPD0a9XPp5QlJRRnX7y5pZfp32SPj+pHp8jc26jNFycHFmY2u75r+/mok9Uyvq0iXfVtScRP7v1Nr//9w4eKcI5S55O+S3gm7xqGRw+1+m97uoiZqA9OAnNPo+oZQPjBLz4ZQviNL/Y60fnKW+jOSaiV1SX8+JF1+RmaQePappaHsu9zZxO2OTLd3lpJnF5ykR5q57oPT213f6PqGUP5BE7d5MK3d3ETthrT2tUbXu3Rbd/f83W7IuO5Loazk4MNJejLLvnwjrf84/bk8/fk1SVbo/w0uXHK98PQ1YnBw+vWvjQvOuY/NbKGk3c2swjm3JqO8UdLfA73fzHL90PTr18zssCbqPZQ89TpQyVPVX02vn+ycqw+sMx9+55w7P9eFnXPTzOwGSbcqeRngE0n/3tSyZraTpCsknSRpDyVP+WbqnWU1bzdx3aL0a2UTtc/Tr32aqM13zjV+qUFKDmZukDQkyzY0aPj7VVjT85d3Sb/uK0nOubVmNknSqZLeTV+Df1XS35xzGwLrAgqGUEY+LVZyJ5jtTj2bhtd5F3v69lPytGRmKC9zzoXmAi/Jcv1O6dcrArfvnH7tmn79PNuCEfiDpJuVPBX9gHOuuvECZtZV0ltKnip+U8lrqyuVPHLtKulSJU89N2VNE9fV5lBr10RtaZZ1NPy9KrLUGzT8/Y5LL9l0zvj+f0m6StJZ+uK1/o1m9pSknznnsm0TUDC8+xr5NC39ekwzb9dwh94rS33XRss1yOXkHNmWaehV4Zwzz2VqulzD3ODmHnAUhJl1kPT79MdVkq43s680segPlQTyTc65I5xzP3bOXeuS6VcTCrO1kpI5601pGANNhXymhvqlgb/fBQ03cM7VOOdudM4NVHKQd7aSMXu2pKdasC9A3hDKyKeHlEyH+raZ7edbsNE0pxnp15FNLLeXkqc/P3PO5fOkGW+kX49q5vInNDVtqAl1kmRmxc3dsK30f5W8zvoLSWdK6ihpQhPTyfZKv35pCpXSaVwF0s+SKXSNjUy/zmiilqm5f79/4Zxb4Jx7VNIJSt7sd2T6tD7Qqghl5I1zbq6SN+qUSnrezJo8Y5eZnSjpzxlXPZh+vdbMdslYrljJNJwiSf+T5829W8kBxB1mNrCJbSw1s3/e4TvnKpW8+/ogJU+BNl5+p/TRaoOq9Gu/vG51E8zs20qmHb2m5A1SL0r6P0pC+o5Gi89Nv45s1GOIkteiC6VY0n9kHuCkU84uUfK09yO+Gzvn3lbymvC30qltX2Jmg82sR/r9LmY2uInFOil5irtWybu+gVbFa8rIK+fcbWZWouTNOm+Z2etK3iDUcJrNEUrmAr+dcZvXzez/SLpS0gfpa3zrlcxTHqTkKcbgvNVmbues9M78QUkfmtkLkj5W8vpnPyWPwJZL2ifjZmcreSPSbWkQTpFk6f4cny47N132JUnfkfQHM/uTkuk585xz43PcxIOyvIGpYftvlP55wpYHlDxlfZZzri5d5Folv+sfmdlLzrmGR8YPK3kd/U4zO1rJG8L2VjK39w9KXncthL9LOkJSpZm9qC/mKXeVdKVz7tMcepyl5M2B/2NmlyiZd71ayTMrBygZO0MlLVPyssMMM3s/XfcCJe/IPkXJU+b/5Zxbl7/dA7ZSa7/9m0vbvCh5w9evlZyoYa2SRyGLlTxC/oGk9k3c5kwlAbxOyTurP5R0jTLm/2YsO1fSXM/6b1TyevLIwHYOVjLlZ56SObUr022+T9KoJpbfSdJ/SPpHuo2rlcyJHSupY8ZyxUqmJs1R8ojcSZqSw+/tfIWnRLl02XZKnsZ1kr7VRK/+SsJ6laQBGdfvp+RkKcuUHPxUKnmteUDaa1yjPuPS6wc0sY6sv+eMfTm/0fVOyQHNbkoeES9Lf5fvKDmwaNxnpJqYp5zWukj6eboP1UoOfj6T9Lyki5TOYVcS9tcrCfHP07/14nQ7viumSXGJ5GLO5fJeGQDIj/Q0pFOdcyNbe1uA2PCaMgAAkSCUAQCIBKEMAEAkeE0ZAIBI8EgZAIBIEMoAAESCUAYAIBKEMgAAkSCUAQCIBKEMAEAkCGUAACJBKAMAEAlCGQCASBDKAABEglAGACAShDIAAJEglAEAiAShDABAJAhlAAAiQSgDABAJQhkAgEgQygAARIJQBgAgEoQyAACRIJQBAIgEoQwAQCQIZQAAIkEoAwAQCUIZAIBIEMoAAESCUAYAIBKEMgAAkSCUAQCIBKFcAGa2l5m51t4OYFthjKMtiGEce0PZzKozLvVmVpPx8/cKtZEZ23O1mX1oZuvMbI6Z/aRRfWGjbfxzM3pn3naJmT1oZp3yvxc5bculZlZpZpvN7IFGtTIze9rM5pmZM7MjW2Mb2wrGeOuM8Yxt2sfMNpnZuNbcju0d4zi+++q0fpGZfZpu65/MbNdQT28oO+c6N1wkzZd0asZ1jzaxASXN2aGtdLakrpJOlnS5mZ3eqD46YxtHN7P36HRfD5M0VNKYxguYWZGZbetnGD6XdLOkcU3UnKRXJJ0lafk23o42jzHeamO8wb2S3izQutosxnF899VmdkxaO0XSTpIWSnok1LBFG2xmt5rZBDP7vZmtk3S2mT1iZjdmLHOsmc3N+LmPmT1jZsvN7DMzuzjX9Tnnfumcm+Gcq3POzZQ0SdLwluxDlvUskPSCpEHpNk8zs1vMbLqk9ZL6mVlXM3vIzBanR243NwwAMys2szvMrMrM5kg6sZnrf8o596yklU3UNjrn7nLOvSapvoW7igDG+LYZ42mPsyUtkTQ1bzuGJjGOC39fLelUSROcczOdc5sk3SpplJn19/XMx1HEaZIek1QhaYJvwfQX8ZyktyT1lnScpCvSIwqZ2dfMbEUuK017HSnpw0alx81smZlNNrPBzdqTL3r3kzRa0oyMq8+R9H1J5UqOeMZLqpG0p6RDlBwNXpAu+yNJx0s6UMmR3BmN+l9jZhO3ZtvQKhjjeR7jZtZV0g2SfrY124+twjgu/H21NfH9IN8N8hHK05xzk5xz9c65msCyQyWVO+duc85tds7NlvQ/ks6UJOfcVOfczjmu9xZJtZIezrjuTEkDJO0uaZqkyWZW0Yx9ec7MVkt6VdJfJP1HRu3B9Ihni6Seko6VdLlzboNzbqmkOxv2Q8kf9g7n3ELnXJWkX2auxDk31jn3zWZsF1oXYzz/Y3yspP92zi1qxrajZRjHhb2vfkHSmWY2yMzKJF2v5OXHjr4b5eN1hQXNWLa/kqcTVmdcVyxpSnNWaGaXKvmlHuWc29xwvXNuWsZit5jZeZKGScr1TQSnOOeybUvmfvaX1F7SUrN/HggVSZqbfr9bo+Xn5bh+xIkxnsjLGDezQyWNkHRZrrdBXjCOEwW5r3bOvWBmt0qaKKmLpP9U8oh9oe92+Qjlxm8fX69/PRLolfH9AkmfOOf23dqVmdlFkn4qaUQOR9lO//r0QUtk7ucCSRskdXfONfW67mJJfTN+7penbUDrYIx/WUvG+Eglj5AWpHeUnSUVm9n+zrnDmrXFaA7G8Zdt0/tq59x/SfovSTKz/SRdJ+kj3222xTvT3pV0spl1s+Tt35dk1KZL2mxmPzWzDumL7IPN7JBcGqdHUzdJOs45N7dRbYCZDTOzdmnvq5W8pjA9rR9rZrV52L+GNxdMlXS7mZVb8i6/vcxsRLrIE5IuM7PeZraTpKua09/MSsysg5Ij0+KG31VGvX1al6TSjO9RGIzxlo3xeyXtJemg9PJbSX+UdFI+th05Yxxvw/tqS6av7m+J/pLuU/JU+Rpfz20RyuMkzVTyNMALkh5vKDjnapX84x2u5OmDFemGlkuSmY1s9HRJY7cqeWt5pX0xv+3utNYl7bVKydvUj1HytvlVab2vpNfysH8NzpbUSclRzypJT+qLI83fSHpJ0vtK3ijxVOYNzew6M5vk6X2jkqc5fibp/PT7zLf8f5pe1zNdT42Z9WnR3qA5xokxvtVjPH1tb0nDRckjthrnHFP8CmucGMfb8r66TMnvtFrSG0oODm4Kbaw5t2OchMeSkxOMd8691NrbAmwLjHG0BTv6ON5hQhkAgNhx7msAACJBKAMAEAlCGQCASBTipORfrKykxLVr1y5rPZfXt0tK/JucS4/6ev8po0P1XBQVhY93Nm7cuMI5t0uLV4ZW1bFjR1dRkf1kRLmMpy1btnjrvv+bXOXyvxFaJpd9WblyJeN6O1dWVuYd03V1dcEeoTFdWlra7O1qLB9jOpd9Wb16dcHGdEFDuV27dhowYEDWem1teGraLrv4fy+59KiurvbWa2pCZ6AL/6HLysqCPWbNmsWZvtqAiooKnXfeeVnrGzZsCPZYvHixt77rrsFPfAsesG7evNlbl6RNmzZ56+vXrw/2+P3vf8+43s5VVFTo3HPPzVpfu3ZtsMeiRf7zhfTpE57FWVxc7K2HxqsUPjhYs8Y7bViS9MwzzxRsTPP0NQAAkSCUAQCIBKEMAEAkCGUAACJBKAMAEAlCGQCASBR0StTGjRv18ccfZ6336NEj2GO//fbz1nOZH7x8uf/DaKqqqoI9QtNcVqxYEeyBtqGqqkoPP/xw1vqqVauy1hoMHTrUW3/rrbeCPebN88/a6NmzZ7DHbrvt5q3n8r+B7d/mzZs1f/78rPWnn3462GPfff0fxfz+++8He8ydO9db79Ah/Km1vXr1Ci4TEx4pAwAQCUIZAIBIEMoAAESCUAYAIBKEMgAAkSCUAQCIBKEMAEAkCGUAACJR8M9T9k3kHjRoULDHqaee6q3vvffewR6hzzrO5TM6KysrvfUXX3wx2OPPf/5zcBnEr7i4WL4PhPd9hniDq666ylsPndRDCo/bO++8M9hj4sSJ3nou+4LtX5cuXTRy5Mis9ffeey/Y48QTT2xRXZK6du3qrU+bNi3Y43e/+523PmfOnGCPQuKRMgAAkSCUAQCIBKEMAEAkCGUAACJBKAMAEAlCGQCASBDKAABEoqDzlLt27apTTjkla/2www4L9vjKV77S4u2oqanx1nv06BHsccQRR3jrnTt3DvZgnnLb0KFDB+2zzz5Z67/4xS+CPdatW+etv/rqq8Eehx9+uLf+85//PNijtLTUW2fM7hjKy8t1/PHHZ62/9dZbwR7nnnuut15VVRXs8c4773jrI0aMCPYInbvivvvuC/Z49tlng8vkC4+UAQCIBKEMAEAkCGUAACJBKAMAEAlCGQCASBDKAABEglAGACAShDIAAJEo6MlDKioqdNJJJ2WtH3zwwcEey5Yt89Y//vjjYI/PP//cWw9NNpfCH/Z+8sknB3ugbejUqZP3xB2+E4s08J2oQZLeeOONYI9hw4Z567fcckuLezz//PPBHtj+tW/fXnvssUfWei5jOnQfecUVVwR7TJkyxVvP5YRTY8eO9dZ9J7RqwMlDAADYARHKAABEglAGACAShDIAAJEglAEAiAShDABAJAhlAAAiUdB5yu3bt9fAgQOz1mtra4M9HnvsMW/99ddfD/YIzXUeMmRIsMfRRx/trZ9wwgnBHmg7ioqyH9+uXr06ePtPP/3UW+/YsWOwx3vvveet33DDDcEeq1at8taLi4uDPdA21NfXZ63lMk959uzZ3nrfvn2DPTp37uytv//++8Eel156qbfeoUOHYI9C4pEyAACRIJQBAIgEoQwAQCQIZQAAIkEoAwAQCUIZAIBIEMoAAESCUAYAIBIFPXlIUVGR2rdvn7VeU1MT7BGaLP7RRx8Fe4ROkLBhw4Zgj5BcTvaAtmHDhg2qrKzMWi8pCf+b3XXXXd76PffcE+zx8ssve+szZ84M9ghta+hkDmg7fCfEKS0tDd7+pZde8tavvPLKYI9+/fp5648++miwx5w5c7z1srKyYI9C4pEyAACRIJQBAIgEoQwAQCQIZQAAIkEoAwAQCUIZAIBIEMoAAESioPOUnXPasmVL1nouHwa/aNEibz2Xuc4hS5cuDS7zyiuveOvLli1r8XZg+7Bu3TrveDjnnHOCPR544AFvfY899gj2uPPOO731yZMnB3tUV1d768xThiQNHDgwuMyFF17orYfOOSFJt99+u7d+7LHHBnuE/i+mTp0a7FFIPFIGACAShDIAAJEglAEAiAShDABAJAhlAAAiQSgDABAJQhkAgEgQygAARKKgJw+pq6vTmjVrstZ33333YI8LLrjAWw+d1EMKT1oPfSi2JO9+SNLcuXODPdA2tGvXTrvuumvW+pQpU4I9vvGNb3jr1157bbDH3Xff7a1fdtllwR5PP/20t96hQ4dgD7R9udxXH3TQQd76pEmTgj0qKyu99TFjxgR73Hvvvd76XXfdFexx8803B5fJFx4pAwAQCUIZAIBIEMoAAESCUAYAIBKEMgAAkSCUAQCIBKEMAEAkCjpPuaamxjtHOJcPUB89erS33qVLl2CP8vJyb72uri7YY/ny5d56VVVVsAfahi1btmjhwoVZ63369An2WLRokbd+6aWXBntcd9113vp//ud/BnusXr3aW588eXKwB9qG+vr6rLWiovDjuT333NNbf/vtt4M9Nm7c6K3nMk95+vTp3vrFF18c7ME8ZQAAdkCEMgAAkSCUAQCIBKEMAEAkCGUAACJBKAMAEAlCGQCASBDKAABEoqAnD6mqqtL48eOz1pcuXRrscf7553vroQ+Ll6T999/fW+/evXuwx+uvv+6tz5o1K9gDbUOnTp00fPjwrPXa2tpgj48++qjF23HllVd66wMGDAj2+M53vuOtf/jhh8EeK1euDC6D+OVyghCfM844w1v/xz/+Eeyxdu1ab33evHnBHpMmTfLWZ8+eHexRSDxSBgAgEoQyAACRIJQBAIgEoQwAQCQIZQAAIkEoAwAQCUIZAIBIFHSe8saNGzVz5sys9S1btgR7FBcXe+sHHHBAsEffvn299dNOOy3Yo1+/ft76hAkTgj3eeOON4DKIX0VFhUaPHp217pvD3CA0x/iDDz4I9jj00EO99a985SvBHqE5m+vWrQv2ACTpwAMP9Na/+c1vBnsMGTLEW//JT34S7DF37lxvvUOHDsEehcQjZQAAIkEoAwAQCUIZAIBIEMoAAESCUAYAIBKEMgAAkSCUAQCIBKEMAEAkCnrykNraWq1atSprfcaMGcEey5cv99ZPOOGEYI9TTjnFW8/lZA8777yzt75ixYpgD04e0jZUVVVp3LhxWeujRo0K9rj//vu99SlTpgR7hE4OksvJeULb0b59+2APtA3OuRbdvmPHjt7666+/Huyx2267eeuPPPJIsEdlZaW3Hrovl6SJEycGl8kXHikDABAJQhkAgEgQygAARIJQBgAgEoQyAACRIJQBAIgEoQwAQCQKOk+5qKjI+4HSRUXhY4Rly5Z56y+//HKwx/z58731XOYYDxo0yFu/7LLLgj3Gjh0bXAbx27Jli5YsWZK1/sMf/jDY47zzzvPWBw8eHOzx2WefeeuhOciStHjxYm+9uro62ANtg5lt0/4DBw4MLnPFFVd46yNHjgz2+PrXv+6tr169OtijkHikDABAJAhlAAAiQSgDABAJQhkAgEgQygAARIJQBgAgEoQyAACRIJQBAIhEQU8eUlxcrO7du2et53LykHXr1nnrCxYsCPYITRYvLS0N9gh9YPw555wT7IG2wczUrl27rPWPP/442OOaa67x1n0n3WlQU1PjrS9dujTYI3RCh7KysmAPtH2bN28OLhO6Hx01alSwx69//WtvfcKECcEeTzzxhLeey/19IfFIGQCASBDKAABEglAGACAShDIAAJEglAEAiAShDABAJAhlAAAiUdB5yh06dNDee++dtZ7LPMrQHGPnXLDHhg0bvPXnnnsu2CM0T3nIkCHBHmgbevXq5f0w9s8++yzYY6+99vLWFy1aFOxRXl7urXfp0iXYY+3atd566DwBknTdddcFl0H86uvrs9byMbf3iCOOCC5z6623euuzZs0K9gjNvV+4cGGwx8033xxcJl94pAwAQCQIZQAAIkEoAwAQCUIZAIBIEMoAAESCUAYAIBKEMgAAkSCUAQCIhOVyso28rcxsuaR5BVth/Po753Zp7Y1AyzCuv4RxvZ1jTH9JwcZ0QUMZAABkx9PXAABEglAGACAShDIAAJEglAEAiAShDABAJAhlAAAiQSgDABAJQhkAgEgQygAARIJQBgAgEoQyAACRIJQBAIgEoQwAQCQIZQAAIkEoAwAQCUIZAIBIEMoAAESCUAYAIBKEMgAAkSCUAQCIBKEMAEAkCGUAACJBKAMAEAlCGQCASBDKBWBme5mZa+3tALYVxjjaghjGsTeUzaw641JvZjUZP3+vUBuZsT1Xm9mHZrbOzOaY2U8a1Rc22sY/N6N35m2XmNmDZtYp/3sR3I4O6brnp/v5jpmdkFEvM7OnzWyemTkzO7LQ29iWMMYLP8bTbZlmZhsz9uPD1tiOtoJx3Grj+FIzqzSzzWb2QKPaVt1Xe0PZOde54SJpvqRTM657tIkNLGnODm2lsyV1lXSypMvN7PRG9dEZ2zi6mb1Hp/t6mKShksY0XsDMisxsWz7DUCpprqSjJFVIuknSU2bWN607Sa9IOkvS8m24HTsExnirjPEG/56xH/sXYH1tFuO41cbx55JuljSuidpW3Ve3aIPN7FYzm2BmvzezdZLONrNHzOzGjGWONbO5GT/3MbNnzGy5mX1mZhfnuj7n3C+dczOcc3XOuZmSJkka3pJ9yLKeBZJekDQo3eZpZnaLmU2XtF5SPzPramYPmdni9Mjt5oYBYGbFZnaHmVWZ2RxJJzZj3Wudczc75+Y55+qdc89KWiDp4LS+0Tl3l3PuNUn1+d1zNMYYz/8YR+ExjrfNOHbOPZXeR69sorZV99X5OIo4TdJjSh7VTfAtmP4inpP0lqTeko6TdIWZHZPWv2ZmK3JZadrrSEmNn/Z63MyWmdlkMxvcrD35onc/SaMlzci4+hxJ35dULmmhpPGSaiTtKekQJUeDF6TL/kjS8ZIOVHIkd0aj/teY2cQct2XXdB0fbc2+IC8Y49tmjP/KzFakd6QjtmY/0CyM4214X503zrmcLkqeUj220XW3Svpro+sekXRjxs/HSpqbfj9c0pxGy18n6be5bkfG7cZKekdSacZ1R0rqIKlT2neRpIoc+y2UVC1ptaR5ku6W1CGtTZN0fcayvZX8kdtnXHeOpP+Xfv+KpB9m1E5KftXN3sdSSS9LuidLfYmkI5vblwtjvLXHuKSvSuosqb2SO9B1kga09hhoCxfGcavcV/9S0gOees731fl4XWFBM5btr+TphNUZ1xVLmtKcFZrZpZLOlHSUc25zw/XOuWkZi91iZudJGiYp1zcRnOKcy7YtmfvZX8mdyVIza7iuSMk/gyTt1mj5eTmu/5/MrFjSo0oG36XNvT3yijGeyNsYd869kfHjg2Z2lpJHPL9pTh80C+M4kdf76nzLRyg3fvv4ekkdM37ulfH9AkmfOOf23dqVmdlFkn4qaYRzblEO22aBZXKVuZ8LJG2Q1N0519RrBYsl9c34uV9zVpQ+3fOQpG6STnbO1TZzW5FfjPEG+HJkAAAar0lEQVQva9EYz7LufO0HmsY4/rJ8j+MW2xbvTHtX0slm1i19PfSSjNp0SZvN7KeWTP0pNrPBZnZILo3To6mbJB3nnJvbqDbAzIaZWbu099VKXlOYntaPNbO8hJtL3lwwVdLtZlZuybv89sp4XewJSZeZWW8z20nSVbn2tuRw7j4lr398wzm3qYll2ptZh/TH0ozvURiM8ZaN8e5mdny6D+3M7Fwl76B9MR/bjpwxjlswjtNtLUnvf4slFTf8rjLqzb6v3hahPE7STCVPA7wg6fGGQvqI7yRJhyt5+mCFkgAqlyQzG9no6ZLGbpW0k6RK+2J+291prUvaa5WSt6kfo+Rt86vSel9Jr+Vh/xqcreT1kI/SdT6pL440fyPpJUnvK3mjxFOZNzSz68xsUpa+e0j6oZJ3Wy/N2M//lbHMp0peJ+mZrqfGzPrkZa+Qi3FijLdkjLeTdJuSaSLLJf27kgPQ2XncdoSNE+O4JeNYkm5Ucl/8M0nnp99nTs9q9n21pS9Ct3lmNk7SeOfcS629LcC2wBhHW7Cjj+MdJpQBAIgd574GACAShDIAAJEglAEAiEQhTkr+T127dnW9evXKWi8pCW/ORx/5zzbZsWNHb12SOnXyf6BIjx49gj3at28fXCaksrJyhXNulxY3QqsqKytz5eXlWesZJy3Iqr5+25/GPB/rKCoKH8cvX76ccb2d23nnnd2AAQOy1mfOnBnsse++Wz3FOTqFvK8uaCj36tVLDzzwQNb6LruE93nwYP8pUgcNGhTsMWzYMG/9xz/+cbDHnnvuGVwmpKioqNXPHoOWKy8v13e/+92s9VwO4NatW5fPTWrSxo0bg8uEgrtLly7BHnfffTfjejs3YMAAvf3221nrhx9+eLDH3/72N289HweruRwkhuTyZudC3lfz9DUAAJEglAEAiAShDABAJAhlAAAiQSgDABAJQhkAgEgUdErUypUr9dhjj2WtP/LII8Ee99xzj7eey3Sma665xlvPZX7dggX+zwvv2bNnsAfahrVr12ry5MlZ66tWrcpaa7Bp05c+ofNf9OkT/hCwpUuXeuu5nAegoqLCW1+zZk2wB9oG31Sh0HQnKTxdyTeNsIEvL6Tc5t6Hlsnl/6KQeKQMAEAkCGUAACJBKAMAEAlCGQCASBDKAABEglAGACAShDIAAJEglAEAiERBZ02Xlpaqb9++Weu5fEbxHXfc4a3/4he/CPa47777vPWLL7442OPNN9/01k899dRgD7QNdXV13pNqdOjQIdjjrLPO8tZPP/30YI/evXt76++9916wx9ixY731QnzuM+KQy+cd++TyOcUhoZOHPP7448Eef/zjH7312traZm3TtsYjZQAAIkEoAwAQCUIZAIBIEMoAAESCUAYAIBKEMgAAkSCUAQCIREHnKTvnVFdXl7U+e/bsYI/q6mpv/e677w72uP766731+++/P9gjNCeUeco7DjNTu3btstZHjRoV7HH55Zd7659//nmwx/PPP++tDx8+PNjj0Ucf9dYvvPDCYI9cthXbt1zm9paU+OMllx6h+fszZswI9nj11Ve99aOOOirYo5B4pAwAQCQIZQAAIkEoAwAQCUIZAIBIEMoAAESCUAYAIBKEMgAAkSCUAQCIREFPHrLrrrtqzJgxWevXXHNNQbZj6dKl3vrcuXODPSorK731ln5AOLYfRUVF6tSpU9b66aefHuyxYsUKb/2yyy4L9vjwww+99e9973vBHnfeeae33qtXr2APtA3Ouay10IlBJHlPFJWvHr/61a+CPerr6731gQMHBnsUEo+UAQCIBKEMAEAkCGUAACJBKAMAEAlCGQCASBDKAABEglAGACASBZ2nLEnFxcUtun1ozllRUfg4IzTX8sYbbwz2CM1D9s3xQ9tSW1urqqqqrPWNGzcGe5SVlXnrixcvDvbo3r27tz558uRgj5EjR3rrucwtRdvgu4/L5f6tpff1+eoRuq+eOXNmsEchxz2PlAEAiAShDABAJAhlAAAiQSgDABAJQhkAgEgQygAARIJQBgAgEoQyAACRiOpMALlMSA+dHKS2tjbYIzQRPDTZPBf56IHtQ1FRkTp06JC1/sorrwR7nHbaad76z372s2CPhx56yFv3neCkwbx587z1Pn36BHug7cvl/i10f57L/X0h7ovzcYKSfOKRMgAAkSCUAQCIBKEMAEAkCGUAACJBKAMAEAlCGQCASBDKAABEIqp5yvmQy4dRh+bH5TI3rr6+vsU90DbU19dr06ZNWesvvfRSsMdVV13lrd9www3BHkOGDPHWr7322mCP1atXe+vz588P9kDbV1dXF1wmNP83H3Od83E/G7ovLzQeKQMAEAlCGQCASBDKAABEglAGACAShDIAAJEglAEAiAShDABAJAhlAAAiEdXJQwp1wo18rKeoiOMZJEpKStStW7es9SVLlgR7TJo0yVtfunRpsMcll1zirT/22GPBHldffbW3XlVVFeyBti90YpB8KUQmxHZfHtfWAACwAyOUAQCIBKEMAEAkCGUAACJBKAMAEAlCGQCASBDKAABEIqp5ysD2aMuWLd65yBUVFcEe1dXV3vpbb70V7HHllVd667fffnuwxw033OCtf/bZZ8EeCxcuDC4DoGk8UgYAIBKEMgAAkSCUAQCIBKEMAEAkCGUAACJBKAMAEAlCGQCASBDKAABEgpOHAC1UUlKiHj16ZK0fcsghwR6zZ8/21hctWhTssWrVKm/9oosuCvaYPn26tz5s2LBgj2nTpgWXAdA0HikDABAJQhkAgEgQygAARIJQBgAgEoQyAACRIJQBAIgEoQwAQCSYpwy0UH19vdavX5+1ftZZZwV79O7d21v//ve/H+yxcuVKb/2AAw4I9igp8d8lLFmyJNgDwNbjkTIAAJEglAEAiAShDABAJAhlAAAiQSgDABAJQhkAgEgQygAARIJQBgAgEpw8BGgh55zq6uqy1p966qlgj7vuustbv++++4I9ZsyY4a2fe+65wR4vvviitz5r1qxgDwBbj0fKAABEglAGACAShDIAAJEglAEAiAShDABAJAhlAAAiQSgDABAJ5ikDLdSuXTv17Nkza/3pp58O9vDNc5akH/3oR8Eew4cP99Zzmes8fvx4bz20nQBahkfKAABEglAGACAShDIAAJEglAEAiAShDABAJAhlAAAiQSgDABAJQhkAgEhw8hCghbZs2aLFixdnre+7777BHs8995y3/sgjjwR7HHbYYd76Bx98EOzRo0cPb33NmjXBHgC2Ho+UAQCIBKEMAEAkCGUAACJBKAMAEAlCGQCASBDKAABEglAGACASzFMGWqhXr1669NJLs9ZXrFgR7HHuued66xs3bgz2WL9+vbf+gx/8INijurraWy8tLQ32uOSSS4LLAGgaj5QBAIgEoQwAQCQIZQAAIkEoAwAQCUIZAIBIEMoAAESCUAYAIBKEMgAAkTDnXOFWZrZc0ryCrTB+/Z1zu7T2RqBlGNdfwrjezjGmv6RgY7qgoQwAALLj6WsAACJBKAMAEAlCGQCASBDKAABEglAGACAShDIAAJEglAEAiAShDABAJAhlAAAiQSgDABAJQhkAgEgQygAARIJQBgAgEoQyAACRIJQBAIgEoQwAQCQIZQAAIkEoAwAQCUIZAIBIEMoAAESCUAYAIBKEMgAAkSCUAQCIBKHcCsxsLzNzrb0dQL4wptEWtca4blYom1l1xqXezGoyfv7ettpIz/ZcbWYfmtk6M5tjZj9pVF/YaBv/3IzembddYmYPmlmn/O9F7sxsHzPbZGbjWnM72hLGNGO6LWJcb7/julmh7Jzr3HCRNF/SqRnXPdrEhpU0d4O2wtmSuko6WdLlZnZ6o/rojG0c3czeo9N9PUzSUEljGi9gZkVmVqhnHO6V9GaB1rVDYEwzptsixvX2O67zuoFmdquZTTCz35vZOklnm9kjZnZjxjLHmtncjJ/7mNkzZrbczD4zs4tzXZ9z7pfOuRnOuTrn3ExJkyQNz+MuNaxngaQXJA1Kt3mamd1iZtMlrZfUz8y6mtlDZrY4PXK7uWEAmFmxmd1hZlVmNkfSic3dBjM7W9ISSVPztmMIYkwzptsixnW843pbHDWcJukxSRWSJvgWTH8Rz0l6S1JvScdJusLMjknrXzOzFbmsNO11pKQPG5UeN7NlZjbZzAY3a0++6N1P0mhJMzKuPkfS9yWVS1ooabykGkl7SjpEydHgBemyP5J0vKQDlRzJndGo/zVmNtGz/q6SbpD0s63ZfrQYY5ox3RYxriMc19silKc55yY55+qdczWBZYdKKnfO3eac2+ycmy3pfySdKUnOuanOuZ1zXO8tkmolPZxx3ZmSBkjaXdI0SZPNrKIZ+/Kcma2W9Kqkv0j6j4zag865mc65LZJ6SjpW0uXOuQ3OuaWS7mzYDyV/2Duccwudc1WSfpm5EufcWOfcNz3bMVbSfzvnFjVj25E/jGnGdFvEuI5wXG+L1xEWNGPZ/kqeTlidcV2xpCnNWaGZXarkl3qUc25zw/XOuWkZi91iZudJGiYp1zcRnOKcy7YtmfvZX1J7SUvNrOG6Iklz0+93a7T8vBzXLzM7VNIISZflehvkHWM6wZhuWxjXiajG9bYI5cZvH18vqWPGz70yvl8g6RPn3L5buzIzu0jSTyWNyOHoxEmywDK5ytzPBZI2SOrunKtvYtnFkvpm/NyvGesZqeTocUE6iDpLKjaz/Z1zhzVri7G1GNNfxpje/jGuv6zVx3Uh3on2rqSTzaybme0q6ZKM2nRJm83sp2bWIX2RfbCZHZJL4/Ro6iZJxznn5jaqDTCzYWbWLu19tZLXFKan9WPNrDYP+9fw5oKpkm43s3JL3uW3l5mNSBd5QtJlZtbbzHaSdFUz2t8raS9JB6WX30r6o6ST8rHt2CqMacZ0W8S4jmBcFyKUx0maqeRpgBckPd5QcM7VKtngw5U8fbBC0n1K/iAys5GNni5p7FZJO0mqtC/mt92d1rqkvVZJ+lzSMUreNr8qrfeV9Foe9q/B2ZI6SfooXeeT+uJI8zeSXpL0vpI3SjyVeUMzu87MJjXVNH3dY0nDRcnRbI1zbnketx3NM06MacZ02zNOjOtWH9fm3I55Eh5LJnWPd8691NrbAuQDYxpt0Y42rnfYUAYAIDac+xoAgEgQygAARIJQBgAgEoU4Cfk/lZWVufLy8qz10tLSYI/QMp06hT8cJJf1FEJlZeUK59wurb0daJmSkhLnG1NlZWXBHr7/C0nq2LGjt57regqBcb39Ky0tdb7xlMt7kWpr/bOYSkoKEz+hba2vb2q68r/asGFDwcZ0QUO5vLxc3/te9k8N69u3b9Zag969e3vrhx9+eLBHaD1FRYV5AqGoqCjns8UgXqWlpRo4cGDW+n777Rfscfzxx3vrQ4YMCfY48MADvfVc7nxCY7+uri7Yo6SkhHG9nSsrK9OwYcOy1jdt2hTssXLlSm99p512CvbIx3jcsmWLt75x48Zgj7fffrtgY5qnrwEAiAShDABAJAhlAAAiQSgDABAJQhkAgEgQygAARKKg574uLi72zn3r0aNHsEf//v299Vx69Ovn/4jMUF2SBg8e7K3vvffewR59+vSpdM4dGlwQUSsqKnIdOnTIWvfVGnTp0qVFdSk89g844IBgj6FDh7aoLkn9+/dnXG/nSkpKnG/MrV27Nthjjz328NZDU6ZyWSY0v18Kn7six+ldBRvTPFIGACAShDIAAJEglAEAiAShDABAJAhlAAAiQSgDABAJQhkAgEgQygAARKKgn6dcUlKinj17Zq0fdNBBwR4777yztz516tRgjyeeeMJbz2U7unXr5q37PosUbUvnzp112GGHZa37Pmu5waeffuqtf/TRR8Ee8+fP99bnzJkT7DFx4kRv/cQTTwz2wPbPzFRSkj0eQp9JL0nnnnuut+7Lggahzzp+7LHHgj3efPPNFm9HIfFIGQCASBDKAABEglAGACAShDIAAJEglAEAiAShDABAJAhlAAAiUdB5yr1799bYsWOz1nP5wOri4mJv/Zxzzgn2eOqpp7z1Z599NtgjNCf09NNPD/ZA21BbW+v9MPbQXEtJ+ta3vuWtf/3rXw/2WL58ubdeWVkZ7PH+++976wMGDAj2wPYvdE6Jq666KthjzZo13vprr70W7DF06FBv/d/+7d+CPZxz3vrMmTODPQqJR8oAAESCUAYAIBKEMgAAkSCUAQCIBKEMAEAkCGUAACJBKAMAEAlCGQCASBT05CEVFRU66aSTstZvu+22YI9FixZ562effXawx2mnneatz5gxI9jjk08+8dZHjBgR7IG2YdOmTZo9e3bW+ooVK4I9/v73v3vruZxY58orr/TW6+rqgj1CJw8ZPXp0sMeYMWOCyyBuZWVlGjRoUNZ6Lidp2nPPPb31OXPmBHu8+eab3vrFF18c7HHQQQd566ExX2g8UgYAIBKEMgAAkSCUAQCIBKEMAEAkCGUAACJBKAMAEAlCGQCASBR0nnJ1dbX3g60ffvjhYI+1a9d665s3bw72uPDCC731/fffP9hj/vz53rpvjh/aFuecNm3alLUemlsvScuWLfPWv/rVrwZ7VFdXe+u5fJi7mXnrufxvoO2bNm1acJnQfXXXrl2DPRYuXOit33fffS3ejtLS0mCPmpqa4DL5wiNlAAAiQSgDABAJQhkAgEgQygAARIJQBgAgEoQyAACRIJQBAIgEoQwAQCQKevIQM1NJSfZVDh48ONgj9KHXb731VrBHUZH/WGTJkiXBHkceeaS3Xl9fH+yBtsN30o3y8vLg7UMnBzn00EODPf76179666H/HUkaNWpUcBm0fRs3btSsWbOy1nv06BHsETpJ0zPPPBPs8fHHH3vroZOLSFK7du289VxOHlJIPFIGACAShDIAAJEglAEAiAShDABAJAhlAAAiQSgDABAJQhkAgEgUdJ5ycXGxd87mt7/97WCPOXPmeOuhD4uXpOnTp3vrW7ZsCfY477zzvPXQXGi0Lb6/d//+/YO3HzFihLfumzPa4JVXXvHWc/nfGD58eHAZtH2bNm3y3tfmcl99//33e+v77LNPsMeDDz7orb/33nvBHps3b/bWY7uvjmtrAADYgRHKAABEglAGACAShDIAAJEglAEAiAShDABAJAhlAAAiQSgDABCJgp48pKqqSuPHj89aP/PMM4M9zjnnHG/97rvvDvaorq721ouLi4M9jj766OAy2DGUlJSoW7duWetXXXVVsMeqVau89eeeey7YY8OGDd56WVlZsMdJJ53krZtZsAe2f6ETPc2ePTvY47vf/a63fsEFFwR7PPPMM976aaedFuwxY8YMb719+/bBHoXEI2UAACJBKAMAEAlCGQCASBDKAABEglAGACAShDIAAJEglAEAiERB5ymvW7dOU6dOzVrfa6+9gj1OPvlkb/31118P9njzzTe99UGDBgV79O3bN7gMdgzdunXzzrEPzR+WpHvvvddb37RpU7BHaH59nz59gj169erlrTNPecdQV1en1atXZ6136dIl2GPt2rXe+j333BPssWLFCm/9ySefDPY45ZRTvPUPP/ww2KOQeKQMAEAkCGUAACJBKAMAEAlCGQCASBDKAABEglAGACAShDIAAJEglAEAiERBTx5SV1enqqqqrPWJEycGexxyyCHeei4fev3JJ59460OHDg324CQKaNCuXTv17Nkza338+PHBHvPnz/fW169fH+xRWlrqrR9xxBHBHiHOuRb3QPzat2+vPfbYo0U9FixY4K3nch/6u9/9zlvPZRuHDx/urYf+96Tc/v/yhUfKAABEglAGACAShDIAAJEglAEAiAShDABAJAhlAAAiQSgDABCJgs5Tds6pvr4+a33evHnBHp9++qm3Xl5eHuzRvXt3b33YsGHBHkCD1atXe+fYf/DBB8Eemzdv9tZDc5AlqajIf4x96qmntrgH85R3DB07dtTBBx+ctX7MMccEe4wdO9ZbX7ZsWbBH7969vfUhQ4YEe8yZM8db37RpU7BHIfFIGQCASBDKAABEglAGACAShDIAAJEglAEAiAShDABAJAhlAAAiQSgDABCJgp48pFu3bjr99NOz1rt06RLscfTRR3vrkydPDvaoqqry1o866qhgj7q6Om+9uLg42ANtQ319vfcEBCUl4X+z6upqb719+/bBHqEPjQ/970jhk4MwrncM69at08svv5y1fuyxxwZ73H777d76n/70p2CPfffd11vP5cQfvhP7SFK7du2CPQqJR8oAAESCUAYAIBKEMgAAkSCUAQCIBKEMAEAkCGUAACJBKAMAEImCzlMuLy/Xcccdl7XeuXPnYI/Kykpv/fHHHw/22G+//bz1ioqKYA8+7B0NunTpoiOPPDJrfdCgQS1ex7vvvhtcpry83FvP5TwARUX+4/TQ/Hy0DfX19dqwYUPW+nXXXRfsEZrLPGTIkGCPf/zjH976o48+GuwRmstcU1MT7FFIPFIGACAShDIAAJEglAEAiAShDABAJAhlAAAiQSgDABAJQhkAgEgQygAARKKgJw9ZtWqVJkyYkLWeywk5Zs2a5a3PnTs32GPMmDHBZULq6+u9dT4MfsexYcMG78k9+vfvH+zRrVs3bz10Ug9JOvHEE7310JjNZZmSkoLeZaCVOOdUW1ubtb5ixYpgjyeffNJb/8Mf/hDs4dsGSVq7dm2wx8477xxcJiY8UgYAIBKEMgAAkSCUAQCIBKEMAEAkCGUAACJBKAMAEAlCGQCASBR00mFtba1Wr16dtf7OO+8Ee4Q+sDoXvg+kl3Kbz8k8ZDSoq6tTdXV11vrUqVODPZYsWeKtd+nSJdhj1KhRwWVCzMxbz+V/A9u/7t2769vf/nbW+meffRbssccee3jrn3/+ebBHaNx37tw52CM0lzk0F1oKz7nOJx4pAwAQCUIZAIBIEMoAAESCUAYAIBKEMgAAkSCUAQCIBKEMAEAkCGUAACJhzrnCrcxsuaR5BVth/Po753Zp7Y1AyzCuv4RxvZ1jTH9JwcZ0QUMZAABkx9PXAABEglAGACAShDIAAJEglAEAiAShDABAJAhlAAAiQSgDABAJQhkAgEgQygAAROL/A9xyH+zs3CyFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the network after training\n",
    "# Accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "feed_dict_test = {x: test_flat_images, y: testLabels}\n",
    "loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_test)\n",
    "print('---------------------------------------------------------')\n",
    "print(\"Test loss: {0:.2f}, test accuracy: {1:.01%}\".format(loss_test, acc_test))\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Plot some of the correct and misclassified examples\n",
    "cls_pred = sess.run(cls_prediction, feed_dict=feed_dict_test)\n",
    "cls_true = np.argmax(testLabels, axis=1)\n",
    "plot_images(test_flat_images, cls_true, cls_pred, title='Correct Examples')\n",
    "plot_example_errors(test_flat_images, cls_true, cls_pred, title='Misclassified Examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the session after you are done with testing\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
