{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from keras.utils import np_utils#one hot\n",
    "import os\n",
    "from skimage import data\n",
    "from skimage import transform\n",
    "import skimage\n",
    "import numpy as np\n",
    "\n",
    "# Download the dataset\n",
    "\n",
    "\n",
    "def load_data(data_directory):\n",
    "    directories = [d for d in os.listdir(data_directory) \n",
    "                  if os.path.isdir(os.path.join(data_directory,d))]\n",
    "    labels=[]\n",
    "    images=[]\n",
    "    for d in directories:\n",
    "        label_directory = os.path.join(data_directory,d)\n",
    "        file_names = [os.path.join(label_directory,f)\n",
    "                     for f in os.listdir(label_directory)\n",
    "                     if f.endswith('.jpg')]\n",
    "        for f in file_names:\n",
    "            images.append(skimage.data.imread(f))\n",
    "            labels.append(str(d))\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ROOT_PATH = '/host//Can/dpr/'\n",
    "train_data_directory = os.path.join(ROOT_PATH, \"outPut3/train\")\n",
    "validation_data_directory = os.path.join(ROOT_PATH, \"outPut3/val\")\n",
    "test_data_directory = os.path.join(ROOT_PATH, \"outPut3/test\")\n",
    "train_images,train_labels = load_data(train_data_directory)\n",
    "test_images,test_labels = load_data(test_data_directory)\n",
    "validation_images,validation_labels = load_data(validation_data_directory)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " 'S',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'Z',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'V',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'G',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'P',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " 'T',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'K',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'Y',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#encoding trY teY\n",
    "from numpy import array\n",
    "#from numpy import argmax\n",
    "#from keras.utils import to_categorical\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define example\n",
    "\n",
    "#train_labels = np_utils.to_categorical(train_labels,num_classes = None)\n",
    "#test_labels = np_utils.to_categorical(test_labels,num_classes = None)\n",
    "trL,teL,valL=array(train_labels),array(test_labels),array(validation_labels)\n",
    "label_encoder = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_encoded_trL = label_encoder.fit_transform(trL)\n",
    "integer_encoded_teL = label_encoder.fit_transform(teL)\n",
    "integer_encoded_valL = label_encoder.fit_transform(valL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels = np_utils.to_categorical(integer_encoded_trL,num_classes = 32)\n",
    "testLabels = np_utils.to_categorical(integer_encoded_teL,num_classes = 32)\n",
    "validationLabels = np_utils.to_categorical(integer_encoded_valL,num_classes = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########convert rgb images into black and white\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "trainImages = rgb2gray(np.array(train_images))\n",
    "testImages = rgb2gray(np.array(test_images))\n",
    "valImages = rgb2gray(np.array(validation_images))\n",
    "train_flat_images = np.reshape(trainImages,(len(trainImages),16*8))\n",
    "test_flat_images = np.reshape(testImages,(len(testImages),16*8))\n",
    "val_flat_images = np.reshape(valImages,(len(valImages),16*8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411, 16, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valImages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_flat_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integer_encoded_trL[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validationLabels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Training-set:\t\t32\n",
      "- Test-set:\t\t411\n",
      "- Validation-set:\t411\n"
     ]
    }
   ],
   "source": [
    "print('Size of:')\n",
    "print('- Training-set:\\t\\t{}'.format(len(trainLabels)))\n",
    "print('- Test-set:\\t\\t{}'.format(len(testLabels)))\n",
    "print('- Validation-set:\\t{}'.format(len(validation_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 8, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "learning_rate = 0.001  # The optimization learning rate\n",
    "epochs = 10  # Total number of training epochs\n",
    "batch_size = 1  # Training batch size\n",
    "display_freq = 10  # Frequency of displaying the training results\n",
    "trainNumber=len(train_labels)\n",
    "testNumber=len(test_labels)\n",
    "# Network Parameters\n",
    "# We know that MNIST images are 28 pixels in each dimension.\n",
    "img_h = 16\n",
    "img_w = 8\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_h * img_w\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "n_classes = 32\n",
    "\n",
    "# number of units in the first hidden layer\n",
    "h1 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight and bais wrappers\n",
    "def weight_variable(name, shape):\n",
    "    \"\"\"\n",
    "    Create a weight variable with appropriate initialization\n",
    "    name: weight name\n",
    "    shape: weight shape\n",
    "\n",
    "    return: initialized weight variable\n",
    "    \"\"\"\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('W_' + name,\n",
    "                           dtype=tf.float32,\n",
    "                           shape=shape,\n",
    "                           initializer=initer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(name, shape):\n",
    "    \"\"\"\n",
    "    Create a bias variable with appropriate initialization\n",
    "    name: bias variable name\n",
    "    shape: bias variable shape\n",
    "\n",
    "    return: initialized bias variable\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b_' + name,\n",
    "                           dtype=tf.float32,\n",
    "                           initializer=initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layer(x, num_nodes, name, use_relu=True):\n",
    "    \"\"\"\n",
    "    Creates a fully-connected layer\n",
    "    :param x: input from previous layer\n",
    "    :param num_nodes: number of hidden units in the fully-connected layer\n",
    "    :param name: layer name\n",
    "    :param use_relu: boolean to add ReLU non-linearity (or not)\n",
    "\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    in_dim = x.get_shape()[1]\n",
    "    W = weight_variable(name, shape=[in_dim, num_nodes])\n",
    "    b = bias_variable(name, [num_nodes])\n",
    "    layer = tf.matmul(x, W)\n",
    "    layer += b\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-1896dc37e3c5>:9: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create graph\n",
    "# Placeholders for inputs (x), outputs(y)\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_classes], name='Y')\n",
    "fc1 = fc_layer(x, h1, 'FC1', use_relu=True)\n",
    "output_logits = fc_layer(fc1, n_classes, 'OUT', use_relu=False)\n",
    "\n",
    "# Define the loss function, optimizer, and accuracy\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam-op').minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "\n",
    "# Network predictions\n",
    "cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "iter   0:\t Loss=3.44,\tTraining Accuracy=100.0%\n",
      "iter  10:\t Loss=3.36,\tTraining Accuracy=100.0%\n",
      "iter  20:\t Loss=3.54,\tTraining Accuracy=0.0%\n",
      "iter  30:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 1, validation loss: 3.47, validation accuracy: 2.7%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 2\n",
      "iter   0:\t Loss=3.61,\tTraining Accuracy=0.0%\n",
      "iter  10:\t Loss=3.44,\tTraining Accuracy=0.0%\n",
      "iter  20:\t Loss=3.69,\tTraining Accuracy=0.0%\n",
      "iter  30:\t Loss=3.41,\tTraining Accuracy=0.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 2, validation loss: 3.47, validation accuracy: 2.9%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 3\n",
      "iter   0:\t Loss=3.33,\tTraining Accuracy=0.0%\n",
      "iter  10:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter  20:\t Loss=3.52,\tTraining Accuracy=0.0%\n",
      "iter  30:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 3, validation loss: 3.47, validation accuracy: 4.4%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 4\n",
      "iter   0:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter  10:\t Loss=3.35,\tTraining Accuracy=0.0%\n",
      "iter  20:\t Loss=3.43,\tTraining Accuracy=0.0%\n",
      "iter  30:\t Loss=3.60,\tTraining Accuracy=0.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 4, validation loss: 3.48, validation accuracy: 4.1%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 5\n",
      "iter   0:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter  10:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "iter  20:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "iter  30:\t Loss=3.49,\tTraining Accuracy=0.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 5, validation loss: 3.46, validation accuracy: 2.7%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 6\n",
      "iter   0:\t Loss=3.53,\tTraining Accuracy=0.0%\n",
      "iter  10:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter  20:\t Loss=3.51,\tTraining Accuracy=0.0%\n",
      "iter  30:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 6, validation loss: 3.46, validation accuracy: 3.2%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 7\n",
      "iter   0:\t Loss=3.39,\tTraining Accuracy=0.0%\n",
      "iter  10:\t Loss=3.28,\tTraining Accuracy=0.0%\n",
      "iter  20:\t Loss=3.22,\tTraining Accuracy=0.0%\n",
      "iter  30:\t Loss=3.31,\tTraining Accuracy=0.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 7, validation loss: 3.49, validation accuracy: 3.4%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 8\n",
      "iter   0:\t Loss=3.08,\tTraining Accuracy=0.0%\n",
      "iter  10:\t Loss=4.06,\tTraining Accuracy=0.0%\n",
      "iter  20:\t Loss=3.45,\tTraining Accuracy=0.0%\n",
      "iter  30:\t Loss=3.40,\tTraining Accuracy=0.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 8, validation loss: 3.46, validation accuracy: 3.2%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 9\n",
      "iter   0:\t Loss=3.57,\tTraining Accuracy=0.0%\n",
      "iter  10:\t Loss=3.36,\tTraining Accuracy=0.0%\n",
      "iter  20:\t Loss=3.67,\tTraining Accuracy=0.0%\n",
      "iter  30:\t Loss=3.65,\tTraining Accuracy=0.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 9, validation loss: 3.46, validation accuracy: 3.2%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 10\n",
      "iter   0:\t Loss=3.70,\tTraining Accuracy=0.0%\n",
      "iter  10:\t Loss=3.66,\tTraining Accuracy=0.0%\n",
      "iter  20:\t Loss=3.38,\tTraining Accuracy=0.0%\n",
      "iter  30:\t Loss=3.55,\tTraining Accuracy=0.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 10, validation loss: 3.45, validation accuracy: 4.6%\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph (session)\n",
    "sess = tf.InteractiveSession() # using InteractiveSession instead of Session to test network in separate cell\n",
    "sess.run(init)\n",
    "# Number of training iterations in each epoch\n",
    "num_tr_iter = int(trainNumber / batch_size)\n",
    "for epoch in range(epochs):\n",
    "    print('Training epoch: {}'.format(epoch+1))\n",
    "    for iteration in range(num_tr_iter):\n",
    "        #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x, batch_y = next_batch(batch_size,train_flat_images,trainLabels)\n",
    "        # Run optimization op (backprop)\n",
    "        feed_dict_batch = {x: batch_x, y: batch_y}\n",
    "        sess.run(optimizer, feed_dict=feed_dict_batch)\n",
    "\n",
    "        if iteration % display_freq == 0:\n",
    "            # Calculate and display the batch loss and accuracy\n",
    "            loss_batch, acc_batch = sess.run([loss, accuracy],\n",
    "                                             feed_dict=feed_dict_batch)\n",
    "            print(\"iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}\".\n",
    "                  format(iteration, loss_batch, acc_batch))\n",
    "\n",
    "    # Run validation after every epoch\n",
    "    feed_dict_valid = {x: val_flat_images, y: validationLabels}\n",
    "    loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
    "          format(epoch + 1, loss_valid, acc_valid))\n",
    "    print('---------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
