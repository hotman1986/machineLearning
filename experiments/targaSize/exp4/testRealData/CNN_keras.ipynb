{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 80\n",
    "\n",
    "WIDTH = 40\n",
    "num_classes = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from keras.utils import np_utils#one hot\n",
    "import os\n",
    "from skimage import data\n",
    "from skimage import transform\n",
    "import skimage\n",
    "import numpy as np\n",
    "\n",
    "# Download the dataset\n",
    "\n",
    "\n",
    "def load_data(data_directory):\n",
    "    directories = [d for d in os.listdir(data_directory) \n",
    "                  if os.path.isdir(os.path.join(data_directory,d))]\n",
    "    labels=[]\n",
    "    images=[]\n",
    "    for d in directories:\n",
    "        label_directory = os.path.join(data_directory,d)\n",
    "        file_names = [os.path.join(label_directory,f)\n",
    "                     for f in os.listdir(label_directory)\n",
    "                     if f.endswith('.jpg')]\n",
    "        for f in file_names:\n",
    "            images.append(skimage.data.imread(f))\n",
    "            labels.append(str(d))\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = '/home/tang/targa/exp4/trainingOriginal/lightTransformation/splitLightData/1SplitDataset/80*40/'\n",
    "train_data_directory = os.path.join(ROOT_PATH, \"train\")\n",
    "train_images,train_labels = load_data(train_data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = '/home/tang/targa//exp4/test_real_data/hongxiang_initial_label/'\n",
    "test_data_directory = os.path.join(TEST_PATH, \"test\")\n",
    "test_images,test_labels = load_data(test_data_directory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding trY teY\n",
    "from numpy import array\n",
    "#from numpy import argmax\n",
    "#from keras.utils import to_categorical\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trL,teL=array(train_labels),array(test_labels)\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_encoded_trL = label_encoder.fit_transform(trL)\n",
    "integer_encoded_teL = label_encoder.fit_transform(teL)\n",
    "trainLabels = np_utils.to_categorical(integer_encoded_trL,num_classes = 32)\n",
    "testLabels = np_utils.to_categorical(integer_encoded_teL,num_classes = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teL[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten,MaxPooling2D\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 40, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=np.asarray(train_images)\n",
    "test = np.asarray(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18169, 80, 40, 3), (63, 80, 40, 3))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#add model layers\n",
    "\n",
    "\n",
    "model.add(Conv2D(2500, padding='same',kernel_size=3,activation=\"relu\", input_shape=(HEIGHT,WIDTH,3)))\n",
    "model.add(MaxPooling2D(pool_size=(4, 2), padding='same', strides=(2, 2)))\n",
    "model.add(Conv2D(1500, padding='same',kernel_size=3, activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), padding='same', strides=(2, 2)))\n",
    "model.add(Conv2D(1000,padding='same', kernel_size=3, activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3),padding='same', strides=(2, 2)))\n",
    "model.add(Conv2D(550,padding='same', kernel_size=2, activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3),padding='same', strides=(2, 2)))\n",
    "model.add(Conv2D(300, padding='same',kernel_size=2, activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(Conv2D(150, padding='same',kernel_size=2, activation=\"relu\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.01),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18169 samples, validate on 63 samples\n",
      "Epoch 1/500\n",
      "18169/18169 [==============================] - 75s 4ms/step - loss: 3.0618 - acc: 0.3821 - val_loss: 3.8861 - val_acc: 0.0635\n",
      "Epoch 2/500\n",
      "18169/18169 [==============================] - 66s 4ms/step - loss: 1.0046 - acc: 0.7641 - val_loss: 3.6359 - val_acc: 0.0952\n",
      "Epoch 3/500\n",
      "18169/18169 [==============================] - 66s 4ms/step - loss: 0.6244 - acc: 0.8418 - val_loss: 3.6552 - val_acc: 0.1111\n",
      "Epoch 4/500\n",
      "18169/18169 [==============================] - 67s 4ms/step - loss: 0.4391 - acc: 0.8855 - val_loss: 3.5197 - val_acc: 0.1587\n",
      "Epoch 5/500\n",
      "18169/18169 [==============================] - 67s 4ms/step - loss: 0.3227 - acc: 0.9191 - val_loss: 3.5144 - val_acc: 0.1587\n",
      "Epoch 6/500\n",
      "18169/18169 [==============================] - 67s 4ms/step - loss: 0.2457 - acc: 0.9419 - val_loss: 3.2423 - val_acc: 0.1429\n",
      "Epoch 7/500\n",
      "18169/18169 [==============================] - 67s 4ms/step - loss: 0.1888 - acc: 0.9557 - val_loss: 3.1627 - val_acc: 0.1587\n",
      "Epoch 8/500\n",
      "18169/18169 [==============================] - 67s 4ms/step - loss: 0.1497 - acc: 0.9661 - val_loss: 3.2867 - val_acc: 0.1587\n",
      "Epoch 9/500\n",
      "18169/18169 [==============================] - 67s 4ms/step - loss: 0.1271 - acc: 0.9722 - val_loss: 3.4010 - val_acc: 0.2063\n",
      "Epoch 10/500\n",
      "18169/18169 [==============================] - 67s 4ms/step - loss: 0.1020 - acc: 0.9786 - val_loss: 3.1130 - val_acc: 0.2222\n",
      "Epoch 11/500\n",
      "  500/18169 [..............................] - ETA: 1:04 - loss: 0.0634 - acc: 0.9880"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "model.fit(train, trainLabels,batch_size=100, validation_data=(test, testLabels), epochs=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
